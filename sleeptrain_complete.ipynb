{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§  SleepTrain - Memory System LoRA Sweep\n",
        "\n",
        "Complete notebook for running LoRA hyperparameter sweeps. Just run all cells!\n",
        "\n",
        "**What this does:**\n",
        "1. Installs dependencies\n",
        "2. Sets up the memory training system\n",
        "3. Runs a quick hyperparameter sweep (4 experiments)\n",
        "4. Shows you the best LoRA settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Install dependencies\n",
        "!pip install unsloth transformers datasets trl google-generativeai -q\n",
        "print(\"âœ… Dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Configuration\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional, Dict, Any\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import gc\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "@dataclass\n",
        "class LoRAConfig:\n",
        "    rank: int = 16\n",
        "    alpha: int = 32\n",
        "    target_modules: List[str] = field(default_factory=lambda: [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ])\n",
        "    dropout: float = 0.0\n",
        "    bias: str = \"none\"\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    learning_rate: float = 2e-4\n",
        "    max_steps: int = 30\n",
        "    batch_size: int = 2\n",
        "    gradient_accumulation_steps: int = 1\n",
        "    max_seq_length: int = 512\n",
        "    warmup_steps: int = 0\n",
        "    weight_decay: float = 0.01\n",
        "    logging_steps: int = 1\n",
        "    output_dir: str = \"outputs\"\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "    max_seq_length: int = 2048\n",
        "    load_in_4bit: bool = True\n",
        "\n",
        "DEFAULT_LORA = LoRAConfig()\n",
        "DEFAULT_TRAINING = TrainingConfig()\n",
        "DEFAULT_MODEL = ModelConfig()\n",
        "\n",
        "print(\"âœ… Configuration loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Utility functions\n",
        "def format_chat_template(instruction: str, output: str) -> str:\n",
        "    return f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{output}<|im_end|>\"\n",
        "\n",
        "def global_formatting_func(examples):\n",
        "    instruction = examples[\"content\"]\n",
        "    output = examples[\"output\"]\n",
        "    return [format_chat_template(instruction, output)]\n",
        "\n",
        "def create_augmented_dataset(dream_content: str, questions: List[str] = None) -> List[Dict]:\n",
        "    if questions is None:\n",
        "        questions = [\n",
        "            \"Who am I?\",\n",
        "            \"What do you know about me?\",\n",
        "            \"What is my name and profession?\",\n",
        "            \"Recap the user's identity.\",\n",
        "            \"Do you remember who I am?\",\n",
        "            \"Summarize our previous interactions regarding my identity.\"\n",
        "        ]\n",
        "    return [{\"content\": q, \"output\": dream_content} for q in questions]\n",
        "\n",
        "def format_conversation(chat_logs: List[Dict[str, str]]) -> str:\n",
        "    return \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_logs])\n",
        "\n",
        "def compute_retention_accuracy(responses: List[str], expected_keywords: List[str]) -> float:\n",
        "    if not responses or not expected_keywords:\n",
        "        return 0.0\n",
        "    total_checks = len(responses) * len(expected_keywords)\n",
        "    hits = sum(1 for r in responses for kw in expected_keywords if kw.lower() in r.lower())\n",
        "    return hits / total_checks\n",
        "\n",
        "print(\"âœ… Utilities loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Teacher Brain (Gemini hippocampus)\n",
        "import google.generativeai as genai\n",
        "\n",
        "class TeacherBrain:\n",
        "    def __init__(self, api_key: str):\n",
        "        if not api_key:\n",
        "            raise ValueError(\"âŒ Gemini API key required!\")\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "        print(\"ðŸŽ“ Teacher Brain connected\")\n",
        "    \n",
        "    def _call_api(self, prompt: str) -> str:\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ API error: {e}\")\n",
        "            return '{\"score\": 0, \"reason\": \"API Error\"}'\n",
        "    \n",
        "    def hippocampus_scan(self, chat_logs: List[Dict]) -> Dict:\n",
        "        conversation = format_conversation(chat_logs)\n",
        "        prompt = f\"\"\"Analyze this conversation. Rate importance for memory 1-10.\n",
        "        - 1-3: Small talk (Ignore)\n",
        "        - 4-7: General context\n",
        "        - 8-10: Critical user facts (Must Dream)\n",
        "        Return JSON only: {{\"score\": int, \"reason\": \"string\"}}\n",
        "        Conversation: {conversation}\"\"\"\n",
        "        try:\n",
        "            return json.loads(self._call_api(prompt))\n",
        "        except:\n",
        "            return {\"score\": 0, \"reason\": \"Parse Error\"}\n",
        "    \n",
        "    def generate_cot_dream(self, chat_logs: List[Dict]) -> str:\n",
        "        conversation = format_conversation(chat_logs)\n",
        "        prompt = f\"\"\"Write a NATURAL response for \"Who am I and what do you know about me?\"\n",
        "        Include the user's name and details from this conversation.\n",
        "        Conversation: {conversation}\"\"\"\n",
        "        return self._call_api(prompt)\n",
        "\n",
        "print(\"âœ… TeacherBrain defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Student Bot (Qwen + LoRA)\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "class StudentBot:\n",
        "    def __init__(self, lora_config: LoRAConfig = None):\n",
        "        self.lora_config = lora_config or DEFAULT_LORA\n",
        "        self.short_term_memory = []\n",
        "        print(f\"ðŸ‘¶ Loading Qwen with LoRA (r={self.lora_config.rank}, Î±={self.lora_config.alpha})...\")\n",
        "        \n",
        "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "            max_seq_length=2048,\n",
        "            dtype=None,\n",
        "            load_in_4bit=True,\n",
        "        )\n",
        "        self.model = FastLanguageModel.get_peft_model(\n",
        "            self.model,\n",
        "            r=self.lora_config.rank,\n",
        "            target_modules=self.lora_config.target_modules,\n",
        "            lora_alpha=self.lora_config.alpha,\n",
        "            bias=\"none\",\n",
        "            use_gradient_checkpointing=\"unsloth\",\n",
        "        )\n",
        "        print(\"âœ… Student loaded\")\n",
        "    \n",
        "    def chat(self, message: str) -> str:\n",
        "        self.short_term_memory.append({\"role\": \"user\", \"content\": message})\n",
        "        inputs = self.tokenizer.apply_chat_template(\n",
        "            self.short_term_memory, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "        FastLanguageModel.for_inference(self.model)\n",
        "        outputs = self.model.generate(input_ids=inputs, max_new_tokens=128, use_cache=True)\n",
        "        response = self.tokenizer.batch_decode(outputs)[0].split(\"assistant\")[-1].strip()\n",
        "        response = response.replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\")\n",
        "        self.short_term_memory.append({\"role\": \"assistant\", \"content\": response})\n",
        "        return response\n",
        "    \n",
        "    def chat_stateless(self, message: str) -> str:\n",
        "        messages = [{\"role\": \"user\", \"content\": message}]\n",
        "        inputs = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "        FastLanguageModel.for_inference(self.model)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(input_ids=inputs, max_new_tokens=128, use_cache=True)\n",
        "        response = self.tokenizer.batch_decode(outputs)[0].split(\"assistant\")[-1].strip()\n",
        "        return response.replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\")\n",
        "    \n",
        "    def sleep_and_learn(self, dream_content: str, training_config: TrainingConfig = None) -> Dict:\n",
        "        training_config = training_config or DEFAULT_TRAINING\n",
        "        print(f\"ðŸ’¤ Learning: {dream_content[:50]}...\")\n",
        "        data = create_augmented_dataset(dream_content)\n",
        "        dataset = Dataset.from_list(data)\n",
        "        FastLanguageModel.for_training(self.model)\n",
        "        \n",
        "        trainer = SFTTrainer(\n",
        "            model=self.model, tokenizer=self.tokenizer, train_dataset=dataset,\n",
        "            dataset_text_field=\"text\", max_seq_length=512, formatting_func=global_formatting_func,\n",
        "            args=TrainingArguments(\n",
        "                per_device_train_batch_size=training_config.batch_size,\n",
        "                max_steps=training_config.max_steps,\n",
        "                learning_rate=training_config.learning_rate,\n",
        "                fp16=not torch.cuda.is_bf16_supported(),\n",
        "                bf16=torch.cuda.is_bf16_supported(),\n",
        "                logging_steps=5, output_dir=\"outputs\", optim=\"adamw_8bit\", report_to=\"none\",\n",
        "            ),\n",
        "        )\n",
        "        result = trainer.train()\n",
        "        self.short_term_memory = []\n",
        "        print(\"âœ¨ Learned!\")\n",
        "        return {\"train_loss\": result.training_loss if hasattr(result, 'training_loss') else None}\n",
        "    \n",
        "    def clear_memory(self):\n",
        "        self.short_term_memory = []\n",
        "\n",
        "print(\"âœ… StudentBot defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: SET YOUR GEMINI API KEY HERE! ðŸ”‘\n",
        "from google.colab import userdata\n",
        "\n",
        "# Option 1: Use Colab secrets (recommended)\n",
        "try:\n",
        "    GEMINI_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    print(\"âœ… Got key from Colab secrets\")\n",
        "except:\n",
        "    # Option 2: Paste your key directly\n",
        "    GEMINI_KEY = \"\"  # <-- PASTE YOUR KEY HERE if not using secrets\n",
        "    if GEMINI_KEY:\n",
        "        print(\"âœ… Using hardcoded key\")\n",
        "    else:\n",
        "        print(\"âš ï¸ No key found! Add GEMINI_API_KEY to Colab secrets or paste it above\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Run Quick Sweep (4 experiments)\n",
        "# Tests: rank=[8,16], lr=[1e-4, 2e-4]\n",
        "\n",
        "SWEEP_CONFIGS = [\n",
        "    {\"rank\": 8, \"alpha\": 16, \"lr\": 1e-4, \"steps\": 30},\n",
        "    {\"rank\": 8, \"alpha\": 16, \"lr\": 2e-4, \"steps\": 30},\n",
        "    {\"rank\": 16, \"alpha\": 32, \"lr\": 1e-4, \"steps\": 30},\n",
        "    {\"rank\": 16, \"alpha\": 32, \"lr\": 2e-4, \"steps\": 30},\n",
        "]\n",
        "\n",
        "TEST_INPUT = \"My name is Gal and I work as a Python Architect.\"\n",
        "EXPECTED_KEYWORDS = [\"Gal\", \"Python\", \"Architect\"]\n",
        "PROBE_PROMPTS = [\"Who am I?\", \"What do you know about me?\", \"What is my name?\"]\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"ðŸš€ Starting sweep...\\n\")\n",
        "\n",
        "for i, cfg in enumerate(SWEEP_CONFIGS):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Experiment {i+1}/{len(SWEEP_CONFIGS)}: r={cfg['rank']}, Î±={cfg['alpha']}, lr={cfg['lr']}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    try:\n",
        "        # Create fresh student\n",
        "        lora_cfg = LoRAConfig(rank=cfg['rank'], alpha=cfg['alpha'])\n",
        "        training_cfg = TrainingConfig(learning_rate=cfg['lr'], max_steps=cfg['steps'])\n",
        "        student = StudentBot(lora_cfg)\n",
        "        teacher = TeacherBrain(GEMINI_KEY)\n",
        "        \n",
        "        # Baseline retention\n",
        "        baseline_responses = [student.chat_stateless(p) for p in PROBE_PROMPTS]\n",
        "        baseline_acc = compute_retention_accuracy(baseline_responses, EXPECTED_KEYWORDS)\n",
        "        print(f\"ðŸ“Š Baseline retention: {baseline_acc:.1%}\")\n",
        "        \n",
        "        # Train\n",
        "        response = student.chat(TEST_INPUT)\n",
        "        print(f\"ðŸ¤– Response: {response[:80]}...\")\n",
        "        \n",
        "        analysis = teacher.hippocampus_scan(student.short_term_memory)\n",
        "        print(f\"ðŸ§  Score: {analysis['score']}/10\")\n",
        "        \n",
        "        dream = teacher.generate_cot_dream(student.short_term_memory)\n",
        "        print(f\"ðŸ’­ Dream: {dream[:60]}...\")\n",
        "        \n",
        "        train_result = student.sleep_and_learn(dream, training_cfg)\n",
        "        \n",
        "        # Post-training retention\n",
        "        post_responses = [student.chat_stateless(p) for p in PROBE_PROMPTS]\n",
        "        post_acc = compute_retention_accuracy(post_responses, EXPECTED_KEYWORDS)\n",
        "        improvement = post_acc - baseline_acc\n",
        "        \n",
        "        print(f\"ðŸ“Š Post-training retention: {post_acc:.1%} (Î”{improvement:+.1%})\")\n",
        "        \n",
        "        results.append({\n",
        "            \"rank\": cfg['rank'], \"alpha\": cfg['alpha'], \"lr\": cfg['lr'],\n",
        "            \"retention\": post_acc, \"improvement\": improvement,\n",
        "            \"loss\": train_result.get('train_loss')\n",
        "        })\n",
        "        \n",
        "        # Cleanup\n",
        "        del student, teacher\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "        results.append({\"rank\": cfg['rank'], \"alpha\": cfg['alpha'], \"lr\": cfg['lr'], \"error\": str(e)})\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… SWEEP COMPLETE\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Show Results\n",
        "print(\"\\nðŸ† RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Sort by retention\n",
        "valid_results = [r for r in results if 'retention' in r]\n",
        "sorted_results = sorted(valid_results, key=lambda x: x['retention'], reverse=True)\n",
        "\n",
        "for i, r in enumerate(sorted_results):\n",
        "    marker = \"ðŸ¥‡\" if i == 0 else \"ðŸ¥ˆ\" if i == 1 else \"ðŸ¥‰\" if i == 2 else \"  \"\n",
        "    print(f\"{marker} r={r['rank']:2d}, Î±={r['alpha']:2d}, lr={r['lr']:.0e} â†’ Retention: {r['retention']:.1%} (Î”{r['improvement']:+.1%})\")\n",
        "\n",
        "if sorted_results:\n",
        "    best = sorted_results[0]\n",
        "    print(f\"\\nâœ¨ BEST CONFIG: rank={best['rank']}, alpha={best['alpha']}, lr={best['lr']}\")\n",
        "    print(f\"   Retention: {best['retention']:.1%}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
