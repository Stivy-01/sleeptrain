{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a555d1",
   "metadata": {},
   "source": [
    "# Ablation Studies: Test Impact of Each Component\n",
    "\n",
    "Studies:\n",
    "1. Hippocampus ON vs OFF\n",
    "2. Interleaved vs Sequential training\n",
    "3. QA-only vs Multi-turn vs Hybrid\n",
    "4. LoRA rank: 8 vs 16 vs 32 vs 64\n",
    "5. Semantic vs Keyword vs Hybrid scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5181292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "!pip install unsloth transformers datasets trl google-generativeai sentence-transformers scikit-learn wandb pandas matplotlib -q\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489dec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add scripts to path\n",
    "scripts_dir = Path.cwd() / \"scripts\"\n",
    "if scripts_dir.exists():\n",
    "    sys.path.insert(0, str(scripts_dir))\n",
    "\n",
    "from scripts.training.hippocampus import create_hippocampus\n",
    "from scripts.training.replay_buffer import PrioritizedReplayBuffer\n",
    "from scripts.evaluation.scoring import SemanticScorer, HybridScorer\n",
    "from scripts.utilities.data_loader import load_people_data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úÖ Modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a024ef7",
   "metadata": {},
   "source": [
    "## Study 1: Hippocampus ON vs OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc83be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study 1: Hippocampus ON vs OFF\n",
    "print(\"=\"*70)\n",
    "print(\"ABLATION STUDY 1: Hippocampus ON vs OFF\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_ablation1 = []\n",
    "\n",
    "# Note: This is a template - actual training code would go here\n",
    "# Run 1: With hippocampus\n",
    "print(\"\\nüß† Run 1: Hippocampus ENABLED\")\n",
    "print(\"   (Training with hippocampus verification enabled)\")\n",
    "# scores_with = evaluate_all()\n",
    "# results_ablation1.append({\"Config\": \"With Hippocampus\", **scores_with})\n",
    "\n",
    "# Run 2: Without hippocampus (store everything)\n",
    "print(\"\\nüß† Run 2: Hippocampus DISABLED\")\n",
    "print(\"   (Training without hippocampus - store all facts)\")\n",
    "# scores_without = evaluate_all()\n",
    "# results_ablation1.append({\"Config\": \"Without Hippocampus\", **scores_without})\n",
    "\n",
    "# Compare\n",
    "if results_ablation1:\n",
    "    df1 = pd.DataFrame(results_ablation1)\n",
    "    print(\"\\nüìä Results:\")\n",
    "    print(df1)\n",
    "    \n",
    "    # Plot\n",
    "    df1.plot(x=\"Config\", y=[\"Single Q\", \"Conversation\", \"Correction\", \"Extended\"], kind=\"bar\")\n",
    "    plt.title(\"Ablation Study 1: Hippocampus Impact\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"ablation_hippocampus.png\")\n",
    "    print(\"‚úÖ Chart saved: ablation_hippocampus.png\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Add training and evaluation code to run this study\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08041d8f",
   "metadata": {},
   "source": [
    "## Study 2: Interleaved vs Sequential Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cdd7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study 2: Interleaved vs Sequential\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ABLATION STUDY 2: Interleaved vs Sequential Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_ablation2 = []\n",
    "\n",
    "# Run 1: Interleaved (current approach)\n",
    "print(\"\\nüîÄ Run 1: INTERLEAVED\")\n",
    "print(\"   (Shuffled training data - prevents catastrophic forgetting)\")\n",
    "# training_data_interleaved = shuffle(training_data)\n",
    "# scores_interleaved = train_and_evaluate(training_data_interleaved)\n",
    "# results_ablation2.append({\"Config\": \"Interleaved\", **scores_interleaved})\n",
    "\n",
    "# Run 2: Sequential (all Obama, then all Musk, then all Curie)\n",
    "print(\"\\n‚û°Ô∏è Run 2: SEQUENTIAL\")\n",
    "print(\"   (All examples for one person, then next person)\")\n",
    "# training_data_sequential = sort_by_person(training_data)\n",
    "# scores_sequential = train_and_evaluate(training_data_sequential)\n",
    "# results_ablation2.append({\"Config\": \"Sequential\", **scores_sequential})\n",
    "\n",
    "# Compare\n",
    "if results_ablation2:\n",
    "    df2 = pd.DataFrame(results_ablation2)\n",
    "    print(\"\\nüìä Results:\")\n",
    "    print(df2)\n",
    "    print(\"\\nüí° Expected: Sequential shows catastrophic forgetting (earlier people forgotten)\")\n",
    "    print(\"   Interleaved should maintain all people equally\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Add training and evaluation code to run this study\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de47910",
   "metadata": {},
   "source": [
    "## Study 3: QA vs Multi-turn vs Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f51de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study 3: QA vs Multi-turn vs Hybrid\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ABLATION STUDY 3: Training Data Format\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_ablation3 = []\n",
    "\n",
    "# Run 1: QA only\n",
    "print(\"\\nüìù Run 1: QA-ONLY\")\n",
    "print(\"   (Single question-answer pairs)\")\n",
    "# qa_data = load_qa_only()\n",
    "# scores_qa = train_and_evaluate(qa_data)\n",
    "# results_ablation3.append({\"Config\": \"QA Only\", **scores_qa})\n",
    "\n",
    "# Run 2: Multi-turn only\n",
    "print(\"\\nüí¨ Run 2: MULTI-TURN ONLY\")\n",
    "print(\"   (Conversation-style training data)\")\n",
    "# multiturn_data = load_multiturn_only()\n",
    "# scores_multiturn = train_and_evaluate(multiturn_data)\n",
    "# results_ablation3.append({\"Config\": \"Multi-turn Only\", **scores_multiturn})\n",
    "\n",
    "# Run 3: Hybrid (50/50 mix)\n",
    "print(\"\\nüîÄ Run 3: HYBRID\")\n",
    "print(\"   (50% QA, 50% multi-turn)\")\n",
    "# hybrid_data = load_hybrid()\n",
    "# scores_hybrid = train_and_evaluate(hybrid_data)\n",
    "# results_ablation3.append({\"Config\": \"Hybrid\", **scores_hybrid})\n",
    "\n",
    "# Compare\n",
    "if results_ablation3:\n",
    "    df3 = pd.DataFrame(results_ablation3)\n",
    "    print(\"\\nüìä Results:\")\n",
    "    print(df3)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Add training and evaluation code to run this study\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb887a4",
   "metadata": {},
   "source": [
    "## Study 4: LoRA Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f089f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study 4: LoRA Rank\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ABLATION STUDY 4: LoRA Rank\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_ablation4 = []\n",
    "\n",
    "for rank in [8, 16, 32, 64]:\n",
    "    print(f\"\\nüîß Testing rank={rank}\")\n",
    "    \n",
    "    # Note: Would reload model with new rank\n",
    "    # model = load_model_with_rank(rank, alpha=rank*2)\n",
    "    # scores = train_and_evaluate(training_data)\n",
    "    \n",
    "    # results_ablation4.append({\n",
    "    #     \"Rank\": rank,\n",
    "    #     \"Alpha\": rank*2,\n",
    "    #     **scores\n",
    "    # })\n",
    "\n",
    "# Compare\n",
    "if results_ablation4:\n",
    "    df4 = pd.DataFrame(results_ablation4)\n",
    "    print(\"\\nüìä Results:\")\n",
    "    print(df4)\n",
    "    \n",
    "    # Plot\n",
    "    df4.plot(x=\"Rank\", y=\"Overall\", kind=\"line\", marker=\"o\")\n",
    "    plt.title(\"Ablation Study 4: LoRA Rank Impact\")\n",
    "    plt.xlabel(\"LoRA Rank\")\n",
    "    plt.ylabel(\"Overall Score\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"ablation_lora_rank.png\")\n",
    "    print(\"‚úÖ Chart saved: ablation_lora_rank.png\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Add training and evaluation code to run this study\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e4b5d",
   "metadata": {},
   "source": [
    "## Study 5: Scoring Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study 5: Scoring Method\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ABLATION STUDY 5: Scoring Method\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load people data\n",
    "PEOPLE = load_people_data(\"configs/people_data.yaml\")\n",
    "\n",
    "# Initialize semantic scorer\n",
    "scorer = SemanticScorer()\n",
    "scorer.precompute_embeddings(PEOPLE)\n",
    "\n",
    "results_ablation5 = []\n",
    "\n",
    "# Note: This study compares scoring methods on the same model outputs\n",
    "# Get sample recalls (would need trained model)\n",
    "# recalls = {p[\"id\"]: recall_person(p) for p in PEOPLE}\n",
    "\n",
    "# Method 1: Keyword\n",
    "print(\"\\nüîë Method 1: KEYWORD\")\n",
    "print(\"   (Simple keyword matching)\")\n",
    "# scores_keyword = {}\n",
    "# for person in PEOPLE:\n",
    "#     scores_keyword[person[\"id\"]] = score_recall_keyword(person, recalls[person[\"id\"]])\n",
    "# avg_keyword = sum(s[\"overall\"] for s in scores_keyword.values()) / len(scores_keyword)\n",
    "# results_ablation5.append({\"Method\": \"Keyword\", \"Average Score\": avg_keyword})\n",
    "\n",
    "# Method 2: Semantic\n",
    "print(\"\\nüß† Method 2: SEMANTIC\")\n",
    "print(\"   (Semantic similarity using embeddings)\")\n",
    "# scores_semantic = {}\n",
    "# for person in PEOPLE:\n",
    "#     scores_semantic[person[\"id\"]] = scorer.score(person, recalls[person[\"id\"]])\n",
    "# avg_semantic = sum(s[\"overall\"] for s in scores_semantic.values()) / len(scores_semantic)\n",
    "# results_ablation5.append({\"Method\": \"Semantic\", \"Average Score\": avg_semantic})\n",
    "\n",
    "# Method 3: Hybrid\n",
    "print(\"\\nüîÄ Method 3: HYBRID (70% semantic)\")\n",
    "print(\"   (Combination of semantic and keyword)\")\n",
    "# hybrid_scorer = HybridScorer(scorer, semantic_weight=0.7)\n",
    "# scores_hybrid = {}\n",
    "# for person in PEOPLE:\n",
    "#     scores_hybrid[person[\"id\"]] = hybrid_scorer.score(person, recalls[person[\"id\"]])\n",
    "# avg_hybrid = sum(s[\"overall\"] for s in scores_hybrid.values()) / len(scores_hybrid)\n",
    "# results_ablation5.append({\"Method\": \"Hybrid\", \"Average Score\": avg_hybrid})\n",
    "\n",
    "# Compare\n",
    "if results_ablation5:\n",
    "    df5 = pd.DataFrame(results_ablation5)\n",
    "    print(\"\\nüìä Results:\")\n",
    "    print(df5)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Add model recall and evaluation code to run this study\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0d2f4",
   "metadata": {},
   "source": [
    "## Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbaa2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä ABLATION STUDIES SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'df1' in locals():\n",
    "    print(\"\\n1. Hippocampus Impact:\")\n",
    "    print(df1)\n",
    "\n",
    "if 'df2' in locals():\n",
    "    print(\"\\n2. Interleaving Impact:\")\n",
    "    print(df2)\n",
    "\n",
    "if 'df3' in locals():\n",
    "    print(\"\\n3. Data Format Impact:\")\n",
    "    print(df3)\n",
    "\n",
    "if 'df4' in locals():\n",
    "    print(\"\\n4. LoRA Rank Impact:\")\n",
    "    print(df4)\n",
    "\n",
    "if 'df5' in locals():\n",
    "    print(\"\\n5. Scoring Method Impact:\")\n",
    "    print(df5)\n",
    "\n",
    "print(\"\\nüèÅ Ablation studies template complete!\")\n",
    "print(\"   Fill in the training and evaluation code to run actual studies.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
