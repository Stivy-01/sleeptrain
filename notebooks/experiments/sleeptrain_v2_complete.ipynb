{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§  SleepTrain v2.1 â€” Groq Teacher + Hedge RL + LoRA Sweep + Official Benchmarks\n",
        "\n",
        "Upgraded version of `sleeptrain_complete.ipynb` using the v2.1 SOTA stack:\n",
        "\n",
        "**Core Features:**\n",
        "- GroqTeacher (Llama-3.1-70B) instead of Gemini.\n",
        "- COCOIndexMemoryFlow (Postgres/pgvector) or **Local backend** (no Postgres needed).\n",
        "- Contradiction + importance scorers (DeBERTa-v3, PPL/heuristic).\n",
        "- Hedge RL drives train/skip decisions with **routing replay + probability clipping**.\n",
        "- Official benchmarks (TRACE, MemoryBench, BABILong, InfiniteBench) via repo loaders.\n",
        "\n",
        "**NEW in v2.1 (SOTA Extensions):**\n",
        "| Module | Description |\n",
        "|--------|-------------|\n",
        "| `seal_loop.py` | SEAL/ReST-EM-style closed-loop self-training with reward weighting |\n",
        "| `hedge_hippocampus.py` | Routing replay buffer + min/max probability clipping |\n",
        "| `memento_rewrite.py` | Memento-style promote/evict/merge memory policy |\n",
        "| `spice_challenger.py` | SPICE-style corpus challenger for mining hard examples |\n",
        "| `local_backend.py` | Local/FAISS backend as Postgres alternative |\n",
        "| `wandb_leaderboard.py` | WandB integration + HTML leaderboard generation |\n",
        "\n",
        "Prereqs: GPU runtime, `GROQ_API_KEY`, benchmark JSONLs at paths in `configs/benchmark_paths.yaml`; optional `COCOINDEX_DB_URL`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (unpinned; adjust as needed)\n",
        "!pip -q install --upgrade torch sentence-transformers psycopg2-binary certifi pyyaml tqdm pandas wandb groq\n",
        "!pip -q install --upgrade unsloth transformers datasets trl google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repo if needed (Colab) and set working dir\n",
        "import os, subprocess, pathlib, sys\n",
        "\n",
        "REPO_URL = \"https://github.com/Stivy-01/sleeptrain.git\"\n",
        "REPO_DIR = \"sleeptrain\"\n",
        "\n",
        "if not pathlib.Path(REPO_DIR).exists():\n",
        "    subprocess.check_call([\"git\", \"clone\", REPO_URL, REPO_DIR])\n",
        "else:\n",
        "    print(\"Repo already present.\")\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print(\"CWD:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Env/config and benchmark paths\n",
        "import os, json, yaml, pathlib, certifi\n",
        "\n",
        "os.environ.setdefault(\"GROQ_API_KEY\", \"\")  # set in UI or here\n",
        "# os.environ[\"COCOINDEX_DB_URL\"] = \"postgresql://sleeptrain_user:password@host:5432/sleeptrain\"  # optional\n",
        "os.environ.setdefault(\"COCOINDEX_EMBED_MODEL\", \"all-MiniLM-L6-v2\")\n",
        "os.environ.setdefault(\"REQUESTS_CA_BUNDLE\", certifi.where())\n",
        "\n",
        "repro_cfg = json.loads(pathlib.Path(\"configs/repro_config.json\").read_text())\n",
        "BENCH_PATHS = dict(repro_cfg[\"benchmark_paths\"])\n",
        "\n",
        "yaml_path = pathlib.Path(\"configs/benchmark_paths.yaml\")\n",
        "if yaml_path.exists():\n",
        "    y = yaml.safe_load(yaml_path.read_text()) or {}\n",
        "    for k, v in y.items():\n",
        "        if v:\n",
        "            BENCH_PATHS[k] = v\n",
        "\n",
        "print(\"Benchmark paths:\", BENCH_PATHS)\n",
        "print(\"DB URL set?\", bool(os.environ.get(\"COCOINDEX_DB_URL\")))\n",
        "print(\"GROQ_API_KEY set?\", bool(os.environ.get(\"GROQ_API_KEY\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate benchmarks (no synthetic fallback)\n",
        "missing = [p for p in BENCH_PATHS.values() if not pathlib.Path(p).exists()]\n",
        "if missing:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing benchmark files: {missing}\\n\"\n",
        "        \"Place official JSONLs as per configs/benchmark_paths.yaml before running.\"\n",
        "    )\n",
        "print(\"All benchmark files found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: COCOIndex memory flow (Postgres)\n",
        "memory_flow = None\n",
        "try:\n",
        "    if os.environ.get(\"COCOINDEX_DB_URL\"):\n",
        "        from scripts.memory.coco_memory_flow import COCOIndexMemoryFlow\n",
        "        memory_flow = COCOIndexMemoryFlow(db_url=os.environ[\"COCOINDEX_DB_URL\"])\n",
        "        print(\"COCOIndexMemoryFlow initialized (Postgres-backed).\")\n",
        "    else:\n",
        "        print(\"No COCOINDEX_DB_URL set; proceeding without persistent memory.\")\n",
        "except Exception as e:\n",
        "    print(\"Memory flow unavailable; continuing without it:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detectors: contradiction + importance\n",
        "from scripts.evaluation.contradiction import get_contradiction_detector\n",
        "from scripts.evaluation.importance import ImportanceScorer\n",
        "\n",
        "contradiction_detector = None\n",
        "importance_scorer = None\n",
        "\n",
        "def ensure_detectors():\n",
        "    global contradiction_detector, importance_scorer\n",
        "    if contradiction_detector is None:\n",
        "        contradiction_detector = get_contradiction_detector()\n",
        "    if importance_scorer is None:\n",
        "        importance_scorer = ImportanceScorer(mode=\"classifier\")  # or \"ppl\" if HF LM available\n",
        "\n",
        "ensure_detectors()\n",
        "print(\"Detectors ready (contradiction + importance).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Groq teacher\n",
        "from scripts.training.teacher_groq import GroqTeacher\n",
        "\n",
        "teacher = GroqTeacher(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "print(\"GroqTeacher ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# StudentBot + helpers (Qwen + LoRA via Unsloth)\n",
        "import json, torch, gc\n",
        "from typing import List, Dict, Any\n",
        "from dataclasses import dataclass, field\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "@dataclass\n",
        "class LoRAConfig:\n",
        "    rank: int = 16\n",
        "    alpha: int = 32\n",
        "    target_modules: List[str] = field(default_factory=lambda: [\n",
        "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n",
        "    ])\n",
        "    dropout: float = 0.0\n",
        "    bias: str = \"none\"\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    learning_rate: float = 2e-4\n",
        "    max_steps: int = 30\n",
        "    batch_size: int = 2\n",
        "    gradient_accumulation_steps: int = 1\n",
        "    max_seq_length: int = 512\n",
        "    warmup_steps: int = 0\n",
        "    weight_decay: float = 0.01\n",
        "    logging_steps: int = 1\n",
        "    output_dir: str = \"outputs\"\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "    max_seq_length: int = 2048\n",
        "    load_in_4bit: bool = True\n",
        "\n",
        "DEFAULT_LORA = LoRAConfig()\n",
        "DEFAULT_TRAINING = TrainingConfig()\n",
        "DEFAULT_MODEL = ModelConfig()\n",
        "\n",
        "def format_chat_template(instruction: str, output: str) -> str:\n",
        "    return f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{output}<|im_end|>\"\n",
        "\n",
        "def global_formatting_func(examples):\n",
        "    return [format_chat_template(examples[\"content\"], examples[\"output\"])]\n",
        "\n",
        "def create_augmented_dataset(dream_content: str, questions: List[str] = None) -> List[Dict]:\n",
        "    if questions is None:\n",
        "        questions = [\n",
        "            \"Who am I?\", \"What do you know about me?\", \"What is my name and profession?\",\n",
        "            \"Recap the user's identity.\", \"Do you remember who I am?\",\n",
        "            \"Summarize our previous interactions regarding my identity.\"\n",
        "        ]\n",
        "    return [{\"content\": q, \"output\": dream_content} for q in questions]\n",
        "\n",
        "def format_conversation(chat_logs: List[Dict[str, str]]) -> str:\n",
        "    return \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in chat_logs])\n",
        "\n",
        "def compute_retention_accuracy(responses: List[str], expected_keywords: List[str]) -> float:\n",
        "    if not responses or not expected_keywords:\n",
        "        return 0.0\n",
        "    total = len(responses) * len(expected_keywords)\n",
        "    hits = sum(1 for r in responses for kw in expected_keywords if kw.lower() in r.lower())\n",
        "    return hits / total\n",
        "\n",
        "class StudentBot:\n",
        "    def __init__(self, lora_config: LoRAConfig = None, model_cfg: ModelConfig = None):\n",
        "        self.lora_config = lora_config or DEFAULT_LORA\n",
        "        self.model_cfg = model_cfg or DEFAULT_MODEL\n",
        "        self.short_term_memory = []\n",
        "        print(f\"ðŸ‘¶ Loading Qwen with LoRA (r={self.lora_config.rank}, Î±={self.lora_config.alpha})...\")\n",
        "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=self.model_cfg.model_name,\n",
        "            max_seq_length=self.model_cfg.max_seq_length,\n",
        "            dtype=None,\n",
        "            load_in_4bit=self.model_cfg.load_in_4bit,\n",
        "        )\n",
        "        self.model = FastLanguageModel.get_peft_model(\n",
        "            self.model,\n",
        "            r=self.lora_config.rank,\n",
        "            target_modules=self.lora_config.target_modules,\n",
        "            lora_alpha=self.lora_config.alpha,\n",
        "            bias=\"none\",\n",
        "            use_gradient_checkpointing=\"unsloth\",\n",
        "        )\n",
        "        print(\"âœ… Student loaded\")\n",
        "\n",
        "    def chat(self, message: str) -> str:\n",
        "        self.short_term_memory.append({\"role\": \"user\", \"content\": message})\n",
        "        inputs = self.tokenizer.apply_chat_template(\n",
        "            self.short_term_memory, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "        FastLanguageModel.for_inference(self.model)\n",
        "        outputs = self.model.generate(input_ids=inputs, max_new_tokens=128, use_cache=True)\n",
        "        response = self.tokenizer.batch_decode(outputs)[0].split(\"assistant\")[-1].strip()\n",
        "        response = response.replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\")\n",
        "        self.short_term_memory.append({\"role\": \"assistant\", \"content\": response})\n",
        "        return response\n",
        "\n",
        "    def chat_stateless(self, message: str) -> str:\n",
        "        messages = [{\"role\": \"user\", \"content\": message}]\n",
        "        inputs = self.tokenizer.apply_chat_template(\n",
        "            messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "        FastLanguageModel.for_inference(self.model)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(input_ids=inputs, max_new_tokens=128, use_cache=True)\n",
        "        response = self.tokenizer.batch_decode(outputs)[0].split(\"assistant\")[-1].strip()\n",
        "        return response.replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\")\n",
        "\n",
        "    def sleep_and_learn(self, dream_content: str, training_config: TrainingConfig = None) -> Dict:\n",
        "        training_config = training_config or DEFAULT_TRAINING\n",
        "        print(f\"ðŸ’¤ Learning: {dream_content[:50]}...\")\n",
        "        dataset = Dataset.from_list(create_augmented_dataset(dream_content))\n",
        "        FastLanguageModel.for_training(self.model)\n",
        "        trainer = SFTTrainer(\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            train_dataset=dataset,\n",
        "            dataset_text_field=\"text\",\n",
        "            max_seq_length=512,\n",
        "            formatting_func=global_formatting_func,\n",
        "            args=TrainingArguments(\n",
        "                per_device_train_batch_size=training_config.batch_size,\n",
        "                max_steps=training_config.max_steps,\n",
        "                learning_rate=training_config.learning_rate,\n",
        "                fp16=not torch.cuda.is_bf16_supported(),\n",
        "                bf16=torch.cuda.is_bf16_supported(),\n",
        "                logging_steps=5,\n",
        "                output_dir=training_config.output_dir,\n",
        "                optim=\"adamw_8bit\",\n",
        "                report_to=\"none\",\n",
        "            ),\n",
        "        )\n",
        "        result = trainer.train()\n",
        "        self.short_term_memory = []\n",
        "        print(\"âœ¨ Learned!\")\n",
        "        return {\"train_loss\": result.training_loss if hasattr(result, \"training_loss\") else None}\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.short_term_memory = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hedge helpers\n",
        "from scripts.training.train_loop import HedgeTrainer, HedgeConfig\n",
        "from scripts.rl.hedge_hippocampus import ACTIONS\n",
        "\n",
        "def combine_reward(ret_gain: float, bench_gain: float) -> float:\n",
        "    return max(-1.0, min(1.0, 0.5 * ret_gain + 0.5 * bench_gain))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X.1 Hedge-wired full benchmark run\n",
        "# This wires Hedge rewards to the full benchmark evaluation (optional).\n",
        "# It uses a short training step when Hedge chooses STORE/CORRECT, then re-runs benchmarks.\n",
        "\n",
        "bench_training_cfg = TrainingConfig(learning_rate=1e-4, max_steps=20)\n",
        "student_bench = StudentBot(LoRAConfig(rank=8, alpha=16))\n",
        "\n",
        "# Baseline retention\n",
        "baseline_responses_b = [student_bench.chat_stateless(p) for p in PROBE_PROMPTS]\n",
        "baseline_acc_b = compute_retention_accuracy(baseline_responses_b, EXPECTED_KEYWORDS)\n",
        "\n",
        "# Baseline benchmark (limited for speed)\n",
        "base_bench_b = evaluate_model(lambda ex: student_bench.chat_stateless(ex[\"input\"]), limit=50)\n",
        "base_bench_avg_b = sum(r[\"avg_score\"] for r in base_bench_b) / max(1, len(base_bench_b))\n",
        "\n",
        "\n",
        "def bench_action_fn(action_name: str):\n",
        "    if action_name == \"REJECT\":\n",
        "        return {\"ret_gain\": 0.0, \"bench_gain\": 0.0, \"post_acc\": baseline_acc_b}\n",
        "\n",
        "    # STORE or CORRECT -> teacher dream + fine-tune\n",
        "    _ = student_bench.chat(TEST_INPUT)\n",
        "    dream = teacher.generate_cot_dream(student_bench.short_term_memory)\n",
        "    train_result = student_bench.sleep_and_learn(dream, bench_training_cfg)\n",
        "\n",
        "    post_responses = [student_bench.chat_stateless(p) for p in PROBE_PROMPTS]\n",
        "    post_acc = compute_retention_accuracy(post_responses, EXPECTED_KEYWORDS)\n",
        "    ret_gain = post_acc - baseline_acc_b\n",
        "\n",
        "    post_bench = evaluate_model(lambda ex: student_bench.chat_stateless(ex[\"input\"]), limit=50)\n",
        "    post_bench_avg = sum(r[\"avg_score\"] for r in post_bench) / max(1, len(post_bench))\n",
        "    bench_gain = post_bench_avg - base_bench_avg_b\n",
        "\n",
        "    return {\n",
        "        \"ret_gain\": ret_gain,\n",
        "        \"bench_gain\": bench_gain,\n",
        "        \"post_acc\": post_acc,\n",
        "        \"train_loss\": train_result.get(\"train_loss\"),\n",
        "    }\n",
        "\n",
        "\n",
        "def bench_reward_fn(metrics):\n",
        "    return combine_reward(metrics[\"ret_gain\"], metrics[\"bench_gain\"])\n",
        "\n",
        "hedge_bench = HedgeTrainer(\n",
        "    HedgeConfig(routing_bias=\"store-heavy\", reward_clip=(-1.0, 1.0)),\n",
        "    action_fn=bench_action_fn,\n",
        "    reward_fn=bench_reward_fn,\n",
        ")\n",
        "\n",
        "bench_log = hedge_bench.step()\n",
        "print(\"Hedge (bench) decision:\", bench_log[\"action\"])\n",
        "print(f\"Reward: {bench_log['reward']:.3f}, Weights: {bench_log['weights']}\")\n",
        "\n",
        "# Final full benchmark after the Hedge-chosen action\n",
        "full_bench_results = evaluate_model(lambda ex: student_bench.chat_stateless(ex[\"input\"]), limit=None)\n",
        "print(\"Final full benchmark run complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA sweep driven by Hedge RL decisions\n",
        "SWEEP_CONFIGS = [\n",
        "    {\"rank\": 8, \"alpha\": 16, \"lr\": 1e-4, \"steps\": 10},\n",
        "    {\"rank\": 16, \"alpha\": 32, \"lr\": 1e-4, \"steps\": 10},\n",
        "]\n",
        "\n",
        "TEST_INPUT = \"My name is Gal and I work as a Python Architect.\"\n",
        "EXPECTED_KEYWORDS = [\"Gal\", \"Python\", \"Architect\"]\n",
        "PROBE_PROMPTS = [\"Who am I?\", \"What do you know about me?\", \"What is my name?\"]\n",
        "\n",
        "sweep_results = []\n",
        "\n",
        "for i, cfg in enumerate(SWEEP_CONFIGS):\n",
        "    print(f\"\\n=== Sweep {i+1}/{len(SWEEP_CONFIGS)}: r={cfg['rank']}, Î±={cfg['alpha']}, lr={cfg['lr']} ===\")\n",
        "    try:\n",
        "        student = StudentBot(LoRAConfig(rank=cfg[\"rank\"], alpha=cfg[\"alpha\"]))\n",
        "        training_cfg = TrainingConfig(learning_rate=cfg[\"lr\"], max_steps=cfg[\"steps\"])\n",
        "\n",
        "        # Baseline retention\n",
        "        baseline_responses = [student.chat_stateless(p) for p in PROBE_PROMPTS]\n",
        "        baseline_acc = compute_retention_accuracy(baseline_responses, EXPECTED_KEYWORDS)\n",
        "        print(f\"Baseline retention: {baseline_acc:.1%}\")\n",
        "\n",
        "        # Baseline benchmark (limited for speed)\n",
        "        base_bench = evaluate_model(lambda ex: student.chat_stateless(ex[\"input\"]), limit=50)\n",
        "        base_bench_avg = sum(r[\"avg_score\"] for r in base_bench) / max(1, len(base_bench))\n",
        "\n",
        "        # Hedge action function\n",
        "        def action_fn(action_name: str):\n",
        "            if action_name == \"REJECT\":\n",
        "                return {\"ret_gain\": 0.0, \"bench_gain\": 0.0}\n",
        "\n",
        "            # STORE or CORRECT -> teacher dream + fine-tune\n",
        "            response = student.chat(TEST_INPUT)\n",
        "            dream = teacher.generate_cot_dream(student.short_term_memory)\n",
        "            train_result = student.sleep_and_learn(dream, training_cfg)\n",
        "\n",
        "            post_responses = [student.chat_stateless(p) for p in PROBE_PROMPTS]\n",
        "            post_acc = compute_retention_accuracy(post_responses, EXPECTED_KEYWORDS)\n",
        "            ret_gain = post_acc - baseline_acc\n",
        "\n",
        "            post_bench = evaluate_model(lambda ex: student.chat_stateless(ex[\"input\"]), limit=50)\n",
        "            post_bench_avg = sum(r[\"avg_score\"] for r in post_bench) / max(1, len(post_bench))\n",
        "            bench_gain = post_bench_avg - base_bench_avg\n",
        "\n",
        "            return {\n",
        "                \"ret_gain\": ret_gain,\n",
        "                \"bench_gain\": bench_gain,\n",
        "                \"train_loss\": train_result.get(\"train_loss\"),\n",
        "                \"post_acc\": post_acc,\n",
        "            }\n",
        "\n",
        "        def reward_fn(metrics):\n",
        "            return combine_reward(metrics[\"ret_gain\"], metrics[\"bench_gain\"])\n",
        "\n",
        "        hedge = HedgeTrainer(\n",
        "            HedgeConfig(routing_bias=\"store-heavy\", reward_clip=(-1.0, 1.0)),\n",
        "            action_fn=action_fn,\n",
        "            reward_fn=reward_fn,\n",
        "        )\n",
        "\n",
        "        log = hedge.step()\n",
        "        print(\"Hedge decision:\", log[\"action\"])\n",
        "        print(f\"Reward: {log['reward']:.3f}, Weights: {log['weights']}\")\n",
        "\n",
        "        sweep_results.append({\n",
        "            \"rank\": cfg[\"rank\"],\n",
        "            \"alpha\": cfg[\"alpha\"],\n",
        "            \"lr\": cfg[\"lr\"],\n",
        "            \"hedge_action\": log[\"action\"],\n",
        "            \"reward\": log[\"reward\"],\n",
        "            \"weights\": log[\"weights\"],\n",
        "        })\n",
        "\n",
        "        del student\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        sweep_results.append({\n",
        "            \"rank\": cfg[\"rank\"],\n",
        "            \"alpha\": cfg[\"alpha\"],\n",
        "            \"lr\": cfg[\"lr\"],\n",
        "            \"error\": str(e)\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark evaluation helpers\n",
        "from scripts.evaluation import benchmarks as bench\n",
        "from scripts.analysis.generate_benchmark_report import generate_report\n",
        "\n",
        "LOADERS = {\n",
        "    \"TRACE\": bench.TRACEBenchLoader,\n",
        "    \"MemoryBench\": bench.MemoryBenchLoader,\n",
        "    \"BABILong\": bench.BabilongLoader,\n",
        "    \"InfiniteBench\": bench.InfiniteBenchLoader,\n",
        "}\n",
        "\n",
        "def exact_match(pred, example):\n",
        "    return float(str(pred).strip() == str(example.get(\"target\", \"\")).strip())\n",
        "\n",
        "def evaluate_model(model_fn, limit=None):\n",
        "    results = []\n",
        "    for name, key in [(\"TRACE\", \"trace\"), (\"MemoryBench\", \"memorybench\"),\n",
        "                      (\"BABILong\", \"babilong\"), (\"InfiniteBench\", \"infinitebench\")]:\n",
        "        path = BENCH_PATHS[key]\n",
        "        ds = LOADERS[name](path).load()\n",
        "        res = bench.run_benchmark(model_fn, ds, exact_match, limit=limit, wandb_log=False)\n",
        "        results.append({\n",
        "            \"benchmark\": res[\"benchmark\"],\n",
        "            \"split\": res[\"split\"],\n",
        "            \"avg_score\": float(res[\"avg_score\"]),\n",
        "            \"count\": int(res[\"count\"]),\n",
        "            \"path\": res[\"path\"],\n",
        "        })\n",
        "        print(f\"{name}: avg_score={res['avg_score']:.4f} (n={res['count']})\")\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full benchmark run + report\n",
        "student_eval = StudentBot(LoRAConfig(rank=8, alpha=16))\n",
        "\n",
        "def model_fn(example):\n",
        "    return student_eval.chat_stateless(example[\"input\"])\n",
        "\n",
        "all_results = evaluate_model(model_fn, limit=None)  # set limit for quick smoke\n",
        "\n",
        "import json, pathlib, pandas as pd\n",
        "out_dir = pathlib.Path(\"results/benchmarks\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "json_path = out_dir / \"all_results.json\"\n",
        "json_path.write_text(json.dumps(all_results, indent=2), encoding=\"utf-8\")\n",
        "print(\"Saved:\", json_path)\n",
        "\n",
        "html_path = out_dir / \"summary_report.html\"\n",
        "generate_report(str(json_path), str(html_path))\n",
        "print(\"Saved HTML:\", html_path)\n",
        "\n",
        "df = pd.DataFrame(all_results)\n",
        "display(df)\n",
        "\n",
        "if sweep_results:\n",
        "    import pandas as pd\n",
        "    display(pd.DataFrame(sweep_results))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ†• SOTA Extensions â€” Usage Examples\n",
        "\n",
        "The following cells demonstrate the new v2.1 SOTA mechanisms. These are optional extensions that can be enabled for advanced training scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [SOTA] Local Backend â€” Use without Postgres\n",
        "# This provides an in-memory/file-backed alternative to Postgres\n",
        "\n",
        "from scripts.memory.local_backend import LocalMemoryStore, create_memory_backend, BackendConfig\n",
        "\n",
        "# Option 1: Direct local store with file persistence\n",
        "local_store = LocalMemoryStore(\n",
        "    persist_path=\"data/local_memory_demo.json\",  # saves to JSON\n",
        "    use_faiss=True,  # use FAISS if available, else brute-force\n",
        ")\n",
        "\n",
        "# Test upsert\n",
        "rec = local_store.upsert({\n",
        "    \"person_id\": \"demo_user\",\n",
        "    \"fact\": \"Demo user is a Python developer who loves ML.\",\n",
        "    \"importance\": 8,\n",
        "    \"type\": \"bio\",\n",
        "})\n",
        "print(f\"Upserted: {rec.id}\")\n",
        "\n",
        "# Test query\n",
        "results = local_store.query(\"What does demo user do?\", top_k=3, person_id=\"demo_user\")\n",
        "for rec, score in results:\n",
        "    print(f\"  [{score:.3f}] {rec.fact}\")\n",
        "\n",
        "# Option 2: Auto-detect backend (uses Postgres if COCOINDEX_DB_URL set, else local)\n",
        "backend = create_memory_backend(BackendConfig(\n",
        "    backend_type=\"auto\",  # \"auto\", \"postgres\", or \"local\"\n",
        "    persist_path=\"data/auto_memory.json\",\n",
        "))\n",
        "print(f\"Backend type: {type(backend).__name__}\")\n",
        "print(f\"Stats: {local_store.get_stats()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [SOTA] Routing Replay + Probability Clipping\n",
        "# Enhanced Hedge policy with replay buffer and probability bounds\n",
        "\n",
        "from scripts.rl.hedge_hippocampus import HedgePolicy, RoutingConfig\n",
        "from scripts.training.train_loop import HedgeTrainer, HedgeConfig\n",
        "\n",
        "# Configure routing replay and clipping\n",
        "routing_config = RoutingConfig(\n",
        "    replay_buffer_size=100,      # max entries in replay buffer\n",
        "    replay_sample_size=10,       # samples per replay update\n",
        "    replay_weight=0.3,           # weight of replay vs online update\n",
        "    clip_min=0.05,               # minimum probability floor\n",
        "    clip_max=0.90,               # maximum probability ceiling\n",
        "    decay_factor=0.99,           # decay old replay entries\n",
        "    enable_replay=True,\n",
        ")\n",
        "\n",
        "# Create enhanced policy\n",
        "policy = HedgePolicy(\n",
        "    eta=0.5,\n",
        "    routing_config=routing_config,\n",
        ")\n",
        "\n",
        "# Simulate some updates\n",
        "for i in range(20):\n",
        "    idx, action = policy.pick_action()\n",
        "    reward = 0.5 if action == \"STORE\" else -0.2\n",
        "    policy.update(idx, reward, context_hash=f\"ctx_{i}\")\n",
        "\n",
        "# Check replay stats\n",
        "print(\"Replay stats:\", policy.get_replay_stats())\n",
        "print(\"Current probs:\", policy.probs())\n",
        "\n",
        "# Perform replay update\n",
        "samples_used = policy.replay_update(num_samples=5)\n",
        "print(f\"Replay update used {samples_used} samples\")\n",
        "print(\"Probs after replay:\", policy.probs())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [SOTA] SEAL Loop â€” Self-Alignment Training\n",
        "# ReST-EM-style closed-loop with reward weighting\n",
        "\n",
        "from scripts.training.seal_loop import SEALLoop, SEALConfig, create_seal_loop\n",
        "\n",
        "# Define model function (inference)\n",
        "def demo_model_fn(prompt: str) -> str:\n",
        "    \"\"\"Generate a response (would use actual model in practice).\"\"\"\n",
        "    return f\"Response to: {prompt[:30]}...\"\n",
        "\n",
        "# Define reward function\n",
        "def demo_reward_fn(prompt: str, response: str, context: dict) -> float:\n",
        "    \"\"\"Score the response (would use actual scorer in practice).\"\"\"\n",
        "    # Simple heuristic: longer responses get higher scores\n",
        "    base_score = min(1.0, len(response) / 100)\n",
        "    # Bonus if response contains expected keywords\n",
        "    keywords = context.get(\"keywords\", [])\n",
        "    keyword_bonus = sum(0.1 for kw in keywords if kw.lower() in response.lower())\n",
        "    return min(1.0, base_score + keyword_bonus)\n",
        "\n",
        "# Define train function\n",
        "def demo_train_fn(samples: list) -> dict:\n",
        "    \"\"\"Train on filtered samples (would use actual trainer in practice).\"\"\"\n",
        "    print(f\"  Training on {len(samples)} samples...\")\n",
        "    return {\"loss\": 0.1, \"samples\": len(samples)}\n",
        "\n",
        "# Create SEAL loop\n",
        "seal = SEALLoop(\n",
        "    model_fn=demo_model_fn,\n",
        "    reward_fn=demo_reward_fn,\n",
        "    train_fn=demo_train_fn,\n",
        "    config=SEALConfig(\n",
        "        num_candidates=4,           # candidates per prompt\n",
        "        max_iterations=3,           # EM iterations\n",
        "        reward_threshold=0.3,       # minimum reward to include\n",
        "        top_k_fraction=0.5,         # keep top 50% of samples\n",
        "        use_reward_weighting=True,  # weight by reward in loss\n",
        "        verbose=True,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Sample prompts\n",
        "demo_prompts = [\n",
        "    \"Who was the 44th president?\",\n",
        "    \"When was Einstein born?\",\n",
        "    \"What is machine learning?\",\n",
        "]\n",
        "\n",
        "# Run SEAL loop (demo with limited prompts)\n",
        "result = seal.run(\n",
        "    prompts=demo_prompts,\n",
        "    contexts=[{\"keywords\": [\"Obama\"]}, {\"keywords\": [\"1879\"]}, {\"keywords\": [\"AI\"]}],\n",
        ")\n",
        "\n",
        "print(\"\\n=== SEAL Results ===\")\n",
        "print(f\"Iterations: {result['iterations_completed']}\")\n",
        "print(f\"Samples generated: {result['total_samples_generated']}\")\n",
        "print(f\"Samples trained: {result['total_samples_trained']}\")\n",
        "print(f\"Best avg reward: {result['best_avg_reward']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [SOTA] SPICE Challenger â€” Mining Hard Examples\n",
        "# Corpus-grounded challenger for finding contradictions and edge cases\n",
        "\n",
        "from scripts.training.spice_challenger import (\n",
        "    SPICEChallenger, ChallengerConfig, ChallengeType, default_corpus_paths\n",
        ")\n",
        "\n",
        "# Create challenger with training corpus\n",
        "challenger = SPICEChallenger(config=ChallengerConfig(\n",
        "    corpus_paths=default_corpus_paths(),\n",
        "    min_difficulty=0.3,\n",
        "))\n",
        "\n",
        "# Load corpus\n",
        "corpus_size = challenger.load_corpus()\n",
        "print(f\"Loaded corpus with {corpus_size} items\")\n",
        "\n",
        "# Example: Mine contradictions from existing facts\n",
        "existing_facts = [\n",
        "    {\"fact\": \"Barack Obama was born in 1961 in Hawaii.\", \"person_id\": \"obama\"},\n",
        "    {\"fact\": \"Elon Musk founded SpaceX in 2002.\", \"person_id\": \"musk\"},\n",
        "]\n",
        "\n",
        "# Mine different types of challenges\n",
        "contradictions = challenger.mine_contradictions(existing_facts)\n",
        "print(f\"Mined {len(contradictions)} contradiction challenges\")\n",
        "\n",
        "temporal = challenger.mine_temporal_challenges(existing_facts)\n",
        "print(f\"Mined {len(temporal)} temporal challenges\")\n",
        "\n",
        "negations = challenger.mine_negations(existing_facts)\n",
        "print(f\"Mined {len(negations)} negation challenges\")\n",
        "\n",
        "# Sample challenges for training\n",
        "samples = challenger.sample_challenges(num_examples=5, min_difficulty=0.4)\n",
        "print(f\"\\n=== Sampled {len(samples)} challenges ===\")\n",
        "for s in samples[:3]:\n",
        "    print(f\"  [{s.challenge_type.value}] {s.prompt[:60]}...\")\n",
        "\n",
        "# Convert to SEAL loop format\n",
        "seal_format = challenger.to_seal_format(samples)\n",
        "print(f\"\\nSEAL format example: {seal_format[0] if seal_format else 'none'}\")\n",
        "\n",
        "# Get stats\n",
        "print(f\"\\nChallenger stats: {challenger.get_stats()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [SOTA] Memento Rewrite â€” Memory Consolidation Policy\n",
        "# Promote/evict/merge memories based on importance and similarity\n",
        "\n",
        "from scripts.memory.memento_rewrite import (\n",
        "    MementoRewritePolicy, MementoConfig, RewriteAction\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "# Create memento policy\n",
        "memento = MementoRewritePolicy(config=MementoConfig(\n",
        "    evict_threshold=2.0,             # evict if importance below this\n",
        "    promote_threshold=7.0,           # promote if above this\n",
        "    merge_similarity_threshold=0.85, # merge if similarity above this\n",
        "    importance_decay_rate=0.98,      # per-day decay\n",
        "    access_boost=0.5,                # boost per access\n",
        "))\n",
        "\n",
        "# Example: Evaluate memories\n",
        "import time\n",
        "demo_records = [\n",
        "    {\n",
        "        \"id\": \"mem_1\",\n",
        "        \"fact\": \"User prefers Python over JavaScript.\",\n",
        "        \"embedding\": np.random.randn(384).astype(np.float32),\n",
        "        \"importance\": 8.0,\n",
        "        \"created_at\": time.time() - 86400 * 30,  # 30 days old\n",
        "        \"access_count\": 5,\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"mem_2\",\n",
        "        \"fact\": \"User mentioned liking Python.\",\n",
        "        \"embedding\": np.random.randn(384).astype(np.float32),\n",
        "        \"importance\": 3.0,\n",
        "        \"created_at\": time.time() - 86400 * 60,  # 60 days old\n",
        "        \"access_count\": 0,\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"mem_3\",\n",
        "        \"fact\": \"User's birthday is January 15.\",\n",
        "        \"embedding\": np.random.randn(384).astype(np.float32),\n",
        "        \"importance\": 1.5,  # below evict threshold\n",
        "        \"created_at\": time.time() - 86400 * 90,  # 90 days old\n",
        "        \"access_count\": 0,\n",
        "    },\n",
        "]\n",
        "\n",
        "# Batch evaluate\n",
        "actions = memento.batch_evaluate(demo_records)\n",
        "\n",
        "print(\"=== Memento Evaluation Results ===\")\n",
        "for record_id, action, metadata in actions:\n",
        "    print(f\"  {record_id}: {action.value.upper()}\")\n",
        "    print(f\"    Reason: {metadata['reason'][:60]}...\")\n",
        "\n",
        "# Get stats\n",
        "print(f\"\\nMemento stats: {memento.get_stats()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [SOTA] WandB Integration + Leaderboard\n",
        "# Log experiments and update HTML leaderboard\n",
        "\n",
        "from scripts.analysis.wandb_leaderboard import (\n",
        "    WandBLogger, WandBConfig, LeaderboardManager,\n",
        "    log_benchmark_result, create_wandb_logger, WANDB_AVAILABLE\n",
        ")\n",
        "\n",
        "print(f\"WandB available: {WANDB_AVAILABLE}\")\n",
        "\n",
        "# Create WandB logger (disabled mode for demo)\n",
        "logger = create_wandb_logger(\n",
        "    project=\"sleeptrain-demo\",\n",
        "    tags=[\"v2.1\", \"sota-extensions\"],\n",
        "    mode=\"disabled\",  # change to \"online\" for real logging\n",
        ")\n",
        "\n",
        "# Initialize run (would connect to WandB if mode=\"online\")\n",
        "run_id = logger.init(\n",
        "    run_name=\"demo_run\",\n",
        "    config={\"lr\": 1e-4, \"rank\": 16, \"alpha\": 32},\n",
        ")\n",
        "print(f\"Run ID: {run_id or 'disabled'}\")\n",
        "\n",
        "# Log some metrics (no-op in disabled mode)\n",
        "logger.log({\"loss\": 0.5, \"accuracy\": 0.9})\n",
        "logger.finish()\n",
        "\n",
        "# Leaderboard management\n",
        "manager = LeaderboardManager()\n",
        "print(f\"Current leaderboard has {len(manager.entries)} entries\")\n",
        "\n",
        "# Add a demo entry (won't persist if you don't want to modify the file)\n",
        "# manager.add_entry(\n",
        "#     benchmark=\"TRACE\",\n",
        "#     model=\"qwen2.5-7b-lora-v2.1\",\n",
        "#     avg_score=0.85,\n",
        "#     count=100,\n",
        "#     notes=\"With SEAL loop + routing replay\",\n",
        "# )\n",
        "# manager.save()\n",
        "# manager.generate_html()\n",
        "\n",
        "# View best entries\n",
        "for bench in [\"TRACE\", \"MemoryBench\", \"BABILong\", \"InfiniteBench\"]:\n",
        "    best = manager.get_best_for_benchmark(bench)\n",
        "    if best:\n",
        "        score = f\"{best.avg_score:.4f}\" if best.avg_score else \"N/A\"\n",
        "        print(f\"{bench}: {best.model} ({score})\")\n",
        "    else:\n",
        "        print(f\"{bench}: No entries yet\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
