{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§  SleepTrain v2 â€” Groq Teacher + Hedge RL + LoRA Sweep + Official Benchmarks\n",
        "\n",
        "Upgraded version of `sleeptrain_complete.ipynb` using the v2 stack:\n",
        "- GroqTeacher (Llama-3.1-70B) instead of Gemini.\n",
        "- COCOIndexMemoryFlow (Postgres/pgvector) optional.\n",
        "- Contradiction + importance scorers (DeBERTa-v3, PPL/heuristic).\n",
        "- Hedge RL drives train/skip decisions during LoRA sweep.\n",
        "- Official benchmarks (TRACE, MemoryBench, BABILong, InfiniteBench) via repo loaders; no synthetic data.\n",
        "- Saves JSON + HTML report using the repo generator.\n",
        "\n",
        "Prereqs: GPU runtime, `GROQ_API_KEY`, benchmark JSONLs at paths in `configs/benchmark_paths.yaml` / `configs/repro_config.json`; optional `COCOINDEX_DB_URL`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (unpinned; adjust as needed)\n",
        "!pip -q install --upgrade torch sentence-transformers psycopg2-binary certifi pyyaml tqdm pandas wandb groq\n",
        "!pip -q install --upgrade unsloth transformers datasets trl google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repo if needed (Colab) and set working dir\n",
        "import os, subprocess, pathlib, sys\n",
        "\n",
        "REPO_URL = \"https://github.com/Stivy-01/sleeptrain.git\"\n",
        "REPO_DIR = \"sleeptrain\"\n",
        "\n",
        "if not pathlib.Path(REPO_DIR).exists():\n",
        "    subprocess.check_call([\"git\", \"clone\", REPO_URL, REPO_DIR])\n",
        "else:\n",
        "    print(\"Repo already present.\")\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print(\"CWD:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Env/config and benchmark paths\n",
        "import os, json, yaml, pathlib, certifi\n",
        "\n",
        "os.environ.setdefault(\"GROQ_API_KEY\", \"\")  # set in UI or here\n",
        "# os.environ[\"COCOINDEX_DB_URL\"] = \"postgresql://sleeptrain_user:password@host:5432/sleeptrain\"  # optional\n",
        "os.environ.setdefault(\"COCOINDEX_EMBED_MODEL\", \"all-MiniLM-L6-v2\")\n",
        "os.environ.setdefault(\"REQUESTS_CA_BUNDLE\", certifi.where())\n",
        "\n",
        "repro_cfg = json.loads(pathlib.Path(\"configs/repro_config.json\").read_text())\n",
        "BENCH_PATHS = dict(repro_cfg[\"benchmark_paths\"])\n",
        "\n",
        "yaml_path = pathlib.Path(\"configs/benchmark_paths.yaml\")\n",
        "if yaml_path.exists():\n",
        "    y = yaml.safe_load(yaml_path.read_text()) or {}\n",
        "    for k, v in y.items():\n",
        "        if v:\n",
        "            BENCH_PATHS[k] = v\n",
        "\n",
        "print(\"Benchmark paths:\", BENCH_PATHS)\n",
        "print(\"DB URL set?\", bool(os.environ.get(\"COCOINDEX_DB_URL\")))\n",
        "print(\"GROQ_API_KEY set?\", bool(os.environ.get(\"GROQ_API_KEY\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate benchmarks (no synthetic fallback)\n",
        "missing = [p for p in BENCH_PATHS.values() if not pathlib.Path(p).exists()]\n",
        "if missing:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing benchmark files: {missing}\\n\"\n",
        "        \"Place official JSONLs as per configs/benchmark_paths.yaml before running.\"\n",
        "    )\n",
        "print(\"All benchmark files found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: COCOIndex memory flow (Postgres)\n",
        "memory_flow = None\n",
        "try:\n",
        "    if os.environ.get(\"COCOINDEX_DB_URL\"):\n",
        "        from scripts.memory.coco_memory_flow import COCOIndexMemoryFlow\n",
        "        memory_flow = COCOIndexMemoryFlow(db_url=os.environ[\"COCOINDEX_DB_URL\"])\n",
        "        print(\"COCOIndexMemoryFlow initialized (Postgres-backed).\")\n",
        "    else:\n",
        "        print(\"No COCOINDEX_DB_URL set; proceeding without persistent memory.\")\n",
        "except Exception as e:\n",
        "    print(\"Memory flow unavailable; continuing without it:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detectors: contradiction + importance\n",
        "from scripts.evaluation.contradiction import get_contradiction_detector\n",
        "from scripts.evaluation.importance import ImportanceScorer\n",
        "\n",
        "contradiction_detector = None\n",
        "importance_scorer = None\n",
        "\n",
        "def ensure_detectors():\n",
        "    global contradiction_detector, importance_scorer\n",
        "    if contradiction_detector is None:\n",
        "        contradiction_detector = get_contradiction_detector()\n",
        "    if importance_scorer is None:\n",
        "        importance_scorer = ImportanceScorer(mode=\"classifier\")  # or \"ppl\" if HF LM available\n",
        "\n",
        "ensure_detectors()\n",
        "print(\"Detectors ready (contradiction + importance).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Groq teacher\n",
        "from scripts.training.teacher_groq import GroqTeacher\n",
        "\n",
        "teacher = GroqTeacher(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "print(\"GroqTeacher ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# StudentBot + helpers (Qwen + LoRA via Unsloth)\n",
        "import json, torch, gc\n",
        "from typing import List, Dict, Any\n",
        "from dataclasses import dataclass, field\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "@dataclass\n",
        "class LoRAConfig:\n",
        "    rank: int = 16\n",
        "    alpha: int = 32\n",
        "    target_modules: List[str] = field(default_factory=lambda: [\n",
        "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n",
        "    ])\n",
        "    dropout: float = 0.0\n",
        "    bias: str = \"none\"\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    learning_rate: float = 2e-4\n",
        "    max_steps: int = 30\n",
        "    batch_size: int = 2\n",
        "    gradient_accumulation_steps: int = 1\n",
        "    max_seq_length: int = 512\n",
        "    warmup_steps: int = 0\n",
        "    weight_decay: float = 0.01\n",
        "    logging_steps: int = 1\n",
        "    output_dir: str = \"outputs\"\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "    max_seq_length: int = 2048\n",
        "    load_in_4bit: bool = True\n",
        "\n",
        "DEFAULT_LORA = LoRAConfig()\n",
        "DEFAULT_TRAINING = TrainingConfig()\n",
        "DEFAULT_MODEL = ModelConfig()\n",
        "\n",
        "def format_chat_template(instruction: str, output: str) -> str:\n",
        "    return f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{output}<|im_end|>\"\n",
        "\n",
        "def global_formatting_func(examples):\n",
        "    return [format_chat_template(examples[\"content\"], examples[\"output\"])]\n",
        "\n",
        "def create_augmented_dataset(dream_content: str, questions: List[str] = None) -> List[Dict]:\n",
        "    if questions is None:\n",
        "        questions = [\n",
        "            \"Who am I?\", \"What do you know about me?\", \"What is my name and profession?\",\n",
        "            \"Recap the user's identity.\", \"Do you remember who I am?\",\n",
        "            \"Summarize our previous interactions regarding my identity.\"\n",
        "        ]\n",
        "    return [{\"content\": q, \"output\": dream_content} for q in questions]\n",
        "\n",
        "def format_conversation(chat_logs: List[Dict[str, str]]) -> str:\n",
        "    return \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in chat_logs])\n",
        "\n",
        "def compute_retention_accuracy(responses: List[str], expected_keywords: List[str]) -> float:\n",
        "    if not responses or not expected_keywords:\n",
        "        return 0.0\n",
        "    total = len(responses) * len(expected_keywords)\n",
        "    hits = sum(1 for r in responses for kw in expected_keywords if kw.lower() in r.lower())\n",
        "    return hits / total\n",
        "\n",
        "class StudentBot:\n",
        "    def __init__(self, lora_config: LoRAConfig = None, model_cfg: ModelConfig = None):\n",
        "        self.lora_config = lora_config or DEFAULT_LORA\n",
        "        self.model_cfg = model_cfg or DEFAULT_MODEL\n",
        "        self.short_term_memory = []\n",
        "        print(f\"ðŸ‘¶ Loading Qwen with LoRA (r={self.lora_config.rank}, Î±={self.lora_config.alpha})...\")\n",
        "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=self.model_cfg.model_name,\n",
        "            max_seq_length=self.model_cfg.max_seq_length,\n",
        "            dtype=None,\n",
        "            load_in_4bit=self.model_cfg.load_in_4bit,\n",
        "        )\n",
        "        self.model = FastLanguageModel.get_peft_model(\n",
        "            self.model,\n",
        "            r=self.lora_config.rank,\n",
        "            target_modules=self.lora_config.target_modules,\n",
        "            lora_alpha=self.lora_config.alpha,\n",
        "            bias=\"none\",\n",
        "            use_gradient_checkpointing=\"unsloth\",\n",
        "        )\n",
        "        print(\"âœ… Student loaded\")\n",
        "\n",
        "    def chat(self, message: str) -> str:\n",
        "        self.short_term_memory.append({\"role\": \"user\", \"content\": message})\n",
        "        inputs = self.tokenizer.apply_chat_template(\n",
        "            self.short_term_memory, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "        FastLanguageModel.for_inference(self.model)\n",
        "        outputs = self.model.generate(input_ids=inputs, max_new_tokens=128, use_cache=True)\n",
        "        response = self.tokenizer.batch_decode(outputs)[0].split(\"assistant\")[-1].strip()\n",
        "        response = response.replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\")\n",
        "        self.short_term_memory.append({\"role\": \"assistant\", \"content\": response})\n",
        "        return response\n",
        "\n",
        "    def chat_stateless(self, message: str) -> str:\n",
        "        messages = [{\"role\": \"user\", \"content\": message}]\n",
        "        inputs = self.tokenizer.apply_chat_template(\n",
        "            messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "        FastLanguageModel.for_inference(self.model)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(input_ids=inputs, max_new_tokens=128, use_cache=True)\n",
        "        response = self.tokenizer.batch_decode(outputs)[0].split(\"assistant\")[-1].strip()\n",
        "        return response.replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\")\n",
        "\n",
        "    def sleep_and_learn(self, dream_content: str, training_config: TrainingConfig = None) -> Dict:\n",
        "        training_config = training_config or DEFAULT_TRAINING\n",
        "        print(f\"ðŸ’¤ Learning: {dream_content[:50]}...\")\n",
        "        dataset = Dataset.from_list(create_augmented_dataset(dream_content))\n",
        "        FastLanguageModel.for_training(self.model)\n",
        "        trainer = SFTTrainer(\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            train_dataset=dataset,\n",
        "            dataset_text_field=\"text\",\n",
        "            max_seq_length=512,\n",
        "            formatting_func=global_formatting_func,\n",
        "            args=TrainingArguments(\n",
        "                per_device_train_batch_size=training_config.batch_size,\n",
        "                max_steps=training_config.max_steps,\n",
        "                learning_rate=training_config.learning_rate,\n",
        "                fp16=not torch.cuda.is_bf16_supported(),\n",
        "                bf16=torch.cuda.is_bf16_supported(),\n",
        "                logging_steps=5,\n",
        "                output_dir=training_config.output_dir,\n",
        "                optim=\"adamw_8bit\",\n",
        "                report_to=\"none\",\n",
        "            ),\n",
        "        )\n",
        "        result = trainer.train()\n",
        "        self.short_term_memory = []\n",
        "        print(\"âœ¨ Learned!\")\n",
        "        return {\"train_loss\": result.training_loss if hasattr(result, \"training_loss\") else None}\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.short_term_memory = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hedge helpers\n",
        "from scripts.training.train_loop import HedgeTrainer, HedgeConfig\n",
        "from scripts.rl.hedge_hippocampus import ACTIONS\n",
        "\n",
        "def combine_reward(ret_gain: float, bench_gain: float) -> float:\n",
        "    return max(-1.0, min(1.0, 0.5 * ret_gain + 0.5 * bench_gain))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X.1 Hedge-wired full benchmark run\n",
        "# This wires Hedge rewards to the full benchmark evaluation (optional).\n",
        "# It uses a short training step when Hedge chooses STORE/CORRECT, then re-runs benchmarks.\n",
        "\n",
        "bench_training_cfg = TrainingConfig(learning_rate=1e-4, max_steps=20)\n",
        "student_bench = StudentBot(LoRAConfig(rank=8, alpha=16))\n",
        "\n",
        "# Baseline retention\n",
        "baseline_responses_b = [student_bench.chat_stateless(p) for p in PROBE_PROMPTS]\n",
        "baseline_acc_b = compute_retention_accuracy(baseline_responses_b, EXPECTED_KEYWORDS)\n",
        "\n",
        "# Baseline benchmark (limited for speed)\n",
        "base_bench_b = evaluate_model(lambda ex: student_bench.chat_stateless(ex[\"input\"]), limit=50)\n",
        "base_bench_avg_b = sum(r[\"avg_score\"] for r in base_bench_b) / max(1, len(base_bench_b))\n",
        "\n",
        "\n",
        "def bench_action_fn(action_name: str):\n",
        "    if action_name == \"REJECT\":\n",
        "        return {\"ret_gain\": 0.0, \"bench_gain\": 0.0, \"post_acc\": baseline_acc_b}\n",
        "\n",
        "    # STORE or CORRECT -> teacher dream + fine-tune\n",
        "    _ = student_bench.chat(TEST_INPUT)\n",
        "    dream = teacher.generate_cot_dream(student_bench.short_term_memory)\n",
        "    train_result = student_bench.sleep_and_learn(dream, bench_training_cfg)\n",
        "\n",
        "    post_responses = [student_bench.chat_stateless(p) for p in PROBE_PROMPTS]\n",
        "    post_acc = compute_retention_accuracy(post_responses, EXPECTED_KEYWORDS)\n",
        "    ret_gain = post_acc - baseline_acc_b\n",
        "\n",
        "    post_bench = evaluate_model(lambda ex: student_bench.chat_stateless(ex[\"input\"]), limit=50)\n",
        "    post_bench_avg = sum(r[\"avg_score\"] for r in post_bench) / max(1, len(post_bench))\n",
        "    bench_gain = post_bench_avg - base_bench_avg_b\n",
        "\n",
        "    return {\n",
        "        \"ret_gain\": ret_gain,\n",
        "        \"bench_gain\": bench_gain,\n",
        "        \"post_acc\": post_acc,\n",
        "        \"train_loss\": train_result.get(\"train_loss\"),\n",
        "    }\n",
        "\n",
        "\n",
        "def bench_reward_fn(metrics):\n",
        "    return combine_reward(metrics[\"ret_gain\"], metrics[\"bench_gain\"])\n",
        "\n",
        "hedge_bench = HedgeTrainer(\n",
        "    HedgeConfig(routing_bias=\"store-heavy\", reward_clip=(-1.0, 1.0)),\n",
        "    action_fn=bench_action_fn,\n",
        "    reward_fn=bench_reward_fn,\n",
        ")\n",
        "\n",
        "bench_log = hedge_bench.step()\n",
        "print(\"Hedge (bench) decision:\", bench_log[\"action\"])\n",
        "print(f\"Reward: {bench_log['reward']:.3f}, Weights: {bench_log['weights']}\")\n",
        "\n",
        "# Final full benchmark after the Hedge-chosen action\n",
        "full_bench_results = evaluate_model(lambda ex: student_bench.chat_stateless(ex[\"input\"]), limit=None)\n",
        "print(\"Final full benchmark run complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA sweep driven by Hedge RL decisions\n",
        "SWEEP_CONFIGS = [\n",
        "    {\"rank\": 8, \"alpha\": 16, \"lr\": 1e-4, \"steps\": 10},\n",
        "    {\"rank\": 16, \"alpha\": 32, \"lr\": 1e-4, \"steps\": 10},\n",
        "]\n",
        "\n",
        "TEST_INPUT = \"My name is Gal and I work as a Python Architect.\"\n",
        "EXPECTED_KEYWORDS = [\"Gal\", \"Python\", \"Architect\"]\n",
        "PROBE_PROMPTS = [\"Who am I?\", \"What do you know about me?\", \"What is my name?\"]\n",
        "\n",
        "sweep_results = []\n",
        "\n",
        "for i, cfg in enumerate(SWEEP_CONFIGS):\n",
        "    print(f\"\\n=== Sweep {i+1}/{len(SWEEP_CONFIGS)}: r={cfg['rank']}, Î±={cfg['alpha']}, lr={cfg['lr']} ===\")\n",
        "    try:\n",
        "        student = StudentBot(LoRAConfig(rank=cfg[\"rank\"], alpha=cfg[\"alpha\"]))\n",
        "        training_cfg = TrainingConfig(learning_rate=cfg[\"lr\"], max_steps=cfg[\"steps\"])\n",
        "\n",
        "        # Baseline retention\n",
        "        baseline_responses = [student.chat_stateless(p) for p in PROBE_PROMPTS]\n",
        "        baseline_acc = compute_retention_accuracy(baseline_responses, EXPECTED_KEYWORDS)\n",
        "        print(f\"Baseline retention: {baseline_acc:.1%}\")\n",
        "\n",
        "        # Baseline benchmark (limited for speed)\n",
        "        base_bench = evaluate_model(lambda ex: student.chat_stateless(ex[\"input\"]), limit=50)\n",
        "        base_bench_avg = sum(r[\"avg_score\"] for r in base_bench) / max(1, len(base_bench))\n",
        "\n",
        "        # Hedge action function\n",
        "        def action_fn(action_name: str):\n",
        "            if action_name == \"REJECT\":\n",
        "                return {\"ret_gain\": 0.0, \"bench_gain\": 0.0}\n",
        "\n",
        "            # STORE or CORRECT -> teacher dream + fine-tune\n",
        "            response = student.chat(TEST_INPUT)\n",
        "            dream = teacher.generate_cot_dream(student.short_term_memory)\n",
        "            train_result = student.sleep_and_learn(dream, training_cfg)\n",
        "\n",
        "            post_responses = [student.chat_stateless(p) for p in PROBE_PROMPTS]\n",
        "            post_acc = compute_retention_accuracy(post_responses, EXPECTED_KEYWORDS)\n",
        "            ret_gain = post_acc - baseline_acc\n",
        "\n",
        "            post_bench = evaluate_model(lambda ex: student.chat_stateless(ex[\"input\"]), limit=50)\n",
        "            post_bench_avg = sum(r[\"avg_score\"] for r in post_bench) / max(1, len(post_bench))\n",
        "            bench_gain = post_bench_avg - base_bench_avg\n",
        "\n",
        "            return {\n",
        "                \"ret_gain\": ret_gain,\n",
        "                \"bench_gain\": bench_gain,\n",
        "                \"train_loss\": train_result.get(\"train_loss\"),\n",
        "                \"post_acc\": post_acc,\n",
        "            }\n",
        "\n",
        "        def reward_fn(metrics):\n",
        "            return combine_reward(metrics[\"ret_gain\"], metrics[\"bench_gain\"])\n",
        "\n",
        "        hedge = HedgeTrainer(\n",
        "            HedgeConfig(routing_bias=\"store-heavy\", reward_clip=(-1.0, 1.0)),\n",
        "            action_fn=action_fn,\n",
        "            reward_fn=reward_fn,\n",
        "        )\n",
        "\n",
        "        log = hedge.step()\n",
        "        print(\"Hedge decision:\", log[\"action\"])\n",
        "        print(f\"Reward: {log['reward']:.3f}, Weights: {log['weights']}\")\n",
        "\n",
        "        sweep_results.append({\n",
        "            \"rank\": cfg[\"rank\"],\n",
        "            \"alpha\": cfg[\"alpha\"],\n",
        "            \"lr\": cfg[\"lr\"],\n",
        "            \"hedge_action\": log[\"action\"],\n",
        "            \"reward\": log[\"reward\"],\n",
        "            \"weights\": log[\"weights\"],\n",
        "        })\n",
        "\n",
        "        del student\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        sweep_results.append({\n",
        "            \"rank\": cfg[\"rank\"],\n",
        "            \"alpha\": cfg[\"alpha\"],\n",
        "            \"lr\": cfg[\"lr\"],\n",
        "            \"error\": str(e)\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark evaluation helpers\n",
        "from scripts.evaluation import benchmarks as bench\n",
        "from scripts.analysis.generate_benchmark_report import generate_report\n",
        "\n",
        "LOADERS = {\n",
        "    \"TRACE\": bench.TRACEBenchLoader,\n",
        "    \"MemoryBench\": bench.MemoryBenchLoader,\n",
        "    \"BABILong\": bench.BabilongLoader,\n",
        "    \"InfiniteBench\": bench.InfiniteBenchLoader,\n",
        "}\n",
        "\n",
        "def exact_match(pred, example):\n",
        "    return float(str(pred).strip() == str(example.get(\"target\", \"\")).strip())\n",
        "\n",
        "def evaluate_model(model_fn, limit=None):\n",
        "    results = []\n",
        "    for name, key in [(\"TRACE\", \"trace\"), (\"MemoryBench\", \"memorybench\"),\n",
        "                      (\"BABILong\", \"babilong\"), (\"InfiniteBench\", \"infinitebench\")]:\n",
        "        path = BENCH_PATHS[key]\n",
        "        ds = LOADERS[name](path).load()\n",
        "        res = bench.run_benchmark(model_fn, ds, exact_match, limit=limit, wandb_log=False)\n",
        "        results.append({\n",
        "            \"benchmark\": res[\"benchmark\"],\n",
        "            \"split\": res[\"split\"],\n",
        "            \"avg_score\": float(res[\"avg_score\"]),\n",
        "            \"count\": int(res[\"count\"]),\n",
        "            \"path\": res[\"path\"],\n",
        "        })\n",
        "        print(f\"{name}: avg_score={res['avg_score']:.4f} (n={res['count']})\")\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full benchmark run + report\n",
        "student_eval = StudentBot(LoRAConfig(rank=8, alpha=16))\n",
        "\n",
        "def model_fn(example):\n",
        "    return student_eval.chat_stateless(example[\"input\"])\n",
        "\n",
        "all_results = evaluate_model(model_fn, limit=None)  # set limit for quick smoke\n",
        "\n",
        "import json, pathlib, pandas as pd\n",
        "out_dir = pathlib.Path(\"results/benchmarks\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "json_path = out_dir / \"all_results.json\"\n",
        "json_path.write_text(json.dumps(all_results, indent=2), encoding=\"utf-8\")\n",
        "print(\"Saved:\", json_path)\n",
        "\n",
        "html_path = out_dir / \"summary_report.html\"\n",
        "generate_report(str(json_path), str(html_path))\n",
        "print(\"Saved HTML:\", html_path)\n",
        "\n",
        "df = pd.DataFrame(all_results)\n",
        "display(df)\n",
        "\n",
        "if sweep_results:\n",
        "    import pandas as pd\n",
        "    display(pd.DataFrame(sweep_results))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
