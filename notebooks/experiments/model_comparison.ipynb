{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5343b68",
   "metadata": {},
   "source": [
    "# Model Comparison: Test Different Model Sizes and Architectures\n",
    "\n",
    "Models to test:\n",
    "- Qwen 1.5B vs 7B vs 14B\n",
    "- Llama 3 8B\n",
    "- Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0137a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "!pip install unsloth transformers datasets trl google-generativeai sentence-transformers scikit-learn pandas matplotlib -q\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81dd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add scripts to path\n",
    "scripts_dir = Path.cwd() / \"scripts\"\n",
    "if scripts_dir.exists():\n",
    "    sys.path.insert(0, str(scripts_dir))\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from scripts.utilities.data_loader import load_people_data\n",
    "\n",
    "print(\"‚úÖ Modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bffaadc",
   "metadata": {},
   "source": [
    "## Test Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeecfbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Configuration\n",
    "MODELS_TO_TEST = [\n",
    "    {\"name\": \"Qwen 1.5B\", \"path\": \"Qwen/Qwen2.5-1.5B-Instruct\", \"rank\": 16},\n",
    "    {\"name\": \"Qwen 7B\", \"path\": \"Qwen/Qwen2.5-7B-Instruct\", \"rank\": 16},\n",
    "    {\"name\": \"Qwen 14B\", \"path\": \"Qwen/Qwen2.5-14B-Instruct\", \"rank\": 32},\n",
    "    {\"name\": \"Llama 3 8B\", \"path\": \"meta-llama/Llama-3-8B-Instruct\", \"rank\": 16},\n",
    "    {\"name\": \"Mistral 7B\", \"path\": \"mistralai/Mistral-7B-Instruct-v0.3\", \"rank\": 16},\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Configured {len(MODELS_TO_TEST)} models to test\")\n",
    "for model in MODELS_TO_TEST:\n",
    "    print(f\"   ‚Ä¢ {model['name']}: {model['path']} (rank={model['rank']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238f987",
   "metadata": {},
   "source": [
    "## Run Model Comparison Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ffd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Model Comparison Tests\n",
    "results_comparison = []\n",
    "\n",
    "# Load people data\n",
    "PEOPLE = load_people_data(\"configs/people_data.yaml\")\n",
    "\n",
    "# Load training data (would need to generate first)\n",
    "# training_data = load_training_data(\"training_data.jsonl\")\n",
    "\n",
    "print(\"‚ö†Ô∏è Template code - fill in training and evaluation functions\")\n",
    "print(\"\\nFor each model:\")\n",
    "print(\"  1. Load model with LoRA\")\n",
    "print(\"  2. Train on same dataset\")\n",
    "print(\"  3. Evaluate on all test types\")\n",
    "print(\"  4. Record metrics (scores, time, memory)\")\n",
    "print(\"  5. Free memory before next model\")\n",
    "\n",
    "# Example structure:\n",
    "# for model_config in MODELS_TO_TEST:\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(f\"TESTING: {model_config['name']}\")\n",
    "#     print(\"=\"*70)\n",
    "#     \n",
    "#     # Load model\n",
    "#     model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#         model_name=model_config['path'],\n",
    "#         max_seq_length=2048,\n",
    "#         dtype=None,\n",
    "#         load_in_4bit=True,\n",
    "#     )\n",
    "#     model = FastLanguageModel.get_peft_model(\n",
    "#         model, r=model_config['rank'], lora_alpha=model_config['rank']*2,\n",
    "#         target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#         lora_dropout=0, bias=\"none\", use_gradient_checkpointing=\"unsloth\",\n",
    "#     )\n",
    "#     \n",
    "#     # Train and evaluate\n",
    "#     train_start = time.time()\n",
    "#     # ... training code ...\n",
    "#     train_time = time.time() - train_start\n",
    "#     \n",
    "#     eval_start = time.time()\n",
    "#     # ... evaluation code ...\n",
    "#     eval_time = time.time() - eval_start\n",
    "#     \n",
    "#     # Record results\n",
    "#     results_comparison.append({\n",
    "#         \"Model\": model_config['name'],\n",
    "#         \"Parameters\": model_config['path'].split('/')[-1],\n",
    "#         \"LoRA Rank\": model_config['rank'],\n",
    "#         \"Train Time (s)\": train_time,\n",
    "#         \"Eval Time (s)\": eval_time,\n",
    "#         \"Single Q\": scores.get(\"single_q_avg\", 0),\n",
    "#         \"Conversation\": scores.get(\"conv_avg\", 0),\n",
    "#         \"Correction\": scores.get(\"correction_avg\", 0),\n",
    "#         \"Extended\": scores.get(\"extended_avg\", 0),\n",
    "#         \"Overall\": scores.get(\"overall_avg\", 0),\n",
    "#         \"Memory (GB)\": torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "#     })\n",
    "#     \n",
    "#     # Free memory\n",
    "#     del model, tokenizer\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab437987",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b3b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Analysis\n",
    "if results_comparison:\n",
    "    df_comparison = pd.DataFrame(results_comparison)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä MODEL COMPARISON RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(df_comparison.to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    df_comparison.to_csv(\"model_comparison_results.csv\", index=False)\n",
    "    print(\"\\n‚úÖ Results saved to model_comparison_results.csv\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Plot 1: Overall scores\n",
    "    df_comparison.plot(x=\"Model\", y=\"Overall\", kind=\"bar\", ax=axes[0,0], title=\"Overall Score\", color=\"steelblue\")\n",
    "    axes[0,0].axhline(y=0.75, color='r', linestyle='--', label='Target (75%)')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].set_ylabel(\"Score\")\n",
    "    \n",
    "    # Plot 2: Test breakdown\n",
    "    df_comparison.plot(x=\"Model\", y=[\"Single Q\", \"Conversation\", \"Correction\", \"Extended\"], kind=\"bar\", ax=axes[0,1], title=\"Score by Test Type\")\n",
    "    axes[0,1].set_ylabel(\"Score\")\n",
    "    \n",
    "    # Plot 3: Training time\n",
    "    df_comparison.plot(x=\"Model\", y=\"Train Time (s)\", kind=\"bar\", ax=axes[0,2], title=\"Training Time\", color=\"orange\")\n",
    "    axes[0,2].set_ylabel(\"Seconds\")\n",
    "    \n",
    "    # Plot 4: Evaluation time\n",
    "    df_comparison.plot(x=\"Model\", y=\"Eval Time (s)\", kind=\"bar\", ax=axes[1,0], title=\"Evaluation Time\", color=\"green\")\n",
    "    axes[1,0].set_ylabel(\"Seconds\")\n",
    "    \n",
    "    # Plot 5: Memory usage\n",
    "    df_comparison.plot(x=\"Model\", y=\"Memory (GB)\", kind=\"bar\", ax=axes[1,1], title=\"GPU Memory Usage\", color=\"purple\")\n",
    "    axes[1,1].set_ylabel(\"GB\")\n",
    "    \n",
    "    # Plot 6: Score vs Memory tradeoff\n",
    "    axes[1,2].scatter(df_comparison[\"Memory (GB)\"], df_comparison[\"Overall\"])\n",
    "    for i, model in enumerate(df_comparison[\"Model\"]):\n",
    "        axes[1,2].annotate(model, (df_comparison[\"Memory (GB)\"][i], df_comparison[\"Overall\"][i]))\n",
    "    axes[1,2].set_xlabel(\"Memory (GB)\")\n",
    "    axes[1,2].set_ylabel(\"Overall Score\")\n",
    "    axes[1,2].set_title(\"Score vs Memory Tradeoff\")\n",
    "    axes[1,2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_comparison_charts.png\", dpi=150)\n",
    "    print(\"‚úÖ Charts saved to model_comparison_charts.png\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üèÜ RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Best overall\n",
    "    best_overall = df_comparison.loc[df_comparison[\"Overall\"].idxmax()]\n",
    "    print(f\"\\n‚úÖ Best overall performance:\")\n",
    "    print(f\"   {best_overall['Model']}: {best_overall['Overall']:.1%}\")\n",
    "    \n",
    "    # Best efficiency (score per GB)\n",
    "    df_comparison[\"Efficiency\"] = df_comparison[\"Overall\"] / df_comparison[\"Memory (GB)\"]\n",
    "    best_efficiency = df_comparison.loc[df_comparison[\"Efficiency\"].idxmax()]\n",
    "    print(f\"\\n‚úÖ Best efficiency (score/GB):\")\n",
    "    print(f\"   {best_efficiency['Model']}: {best_efficiency['Efficiency']:.3f}\")\n",
    "    \n",
    "    # Fastest\n",
    "    fastest = df_comparison.loc[df_comparison[\"Train Time (s)\"].idxmin()]\n",
    "    print(f\"\\n‚úÖ Fastest training:\")\n",
    "    print(f\"   {fastest['Model']}: {fastest['Train Time (s)']:.0f}s\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results yet - run the model comparison tests above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b8caa0",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6251f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèÅ MODEL COMPARISON COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüí° This notebook compares different model architectures and sizes:\")\n",
    "print(\"   ‚Ä¢ Different model families (Qwen, Llama, Mistral)\")\n",
    "print(\"   ‚Ä¢ Different model sizes (1.5B, 7B, 8B, 14B)\")\n",
    "print(\"   ‚Ä¢ Performance vs efficiency tradeoffs\")\n",
    "print(\"   ‚Ä¢ Memory usage analysis\")\n",
    "\n",
    "print(\"\\nüìä Fill in the training and evaluation code to run actual comparisons\")\n",
    "print(\"   Results will help identify the best model for your use case\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
