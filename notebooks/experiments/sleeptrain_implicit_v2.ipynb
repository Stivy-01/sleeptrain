{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies - ADD THIS LINE\n",
    "# Unsloth for fast LoRA training\n",
    "!pip install unsloth transformers datasets trl google-generativeai sentence-transformers scikit-learn -q\n",
    "print(\"‚úÖ Dependencies installed (including sentence-transformers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration + Model Loading - FIXED HYPERPARAMETERS (REPLACE LINES 15-20)\n",
    "import torch\n",
    "import json\n",
    "import gc\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# ============ LOAD PEOPLE DATA FROM YAML ============\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "def load_people_config(config_path=\"configs/people_data.yaml\"):\n",
    "    \"\"\"Load people data from YAML config.\"\"\"\n",
    "    # Check if file exists\n",
    "    if not Path(config_path).exists():\n",
    "        print(f\"‚ö†Ô∏è Config file not found: {config_path}\")\n",
    "        print(f\"   Using hardcoded PEOPLE data\")\n",
    "        return None\n",
    "    \n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    \n",
    "    return data.get(\"people\", [])\n",
    "\n",
    "\n",
    "def convert_yaml_to_people_list(yaml_data):\n",
    "    \"\"\"Convert YAML format to PEOPLE list format for notebooks.\"\"\"\n",
    "    people_list = []\n",
    "    \n",
    "    for person in yaml_data:\n",
    "        # Build facts list from nested structure\n",
    "        facts = []\n",
    "        \n",
    "        # Extract birth info\n",
    "        if \"birth\" in person[\"facts\"]:\n",
    "            birth = person[\"facts\"][\"birth\"]\n",
    "            facts.append({\n",
    "                \"category\": \"birth_date\",\n",
    "                \"fact\": f\"I was born on {birth['date']}.\",\n",
    "                \"key\": str(birth[\"year\"])\n",
    "            })\n",
    "            facts.append({\n",
    "                \"category\": \"birth_place\",\n",
    "                \"fact\": f\"I was born in {birth['place']}.\",\n",
    "                \"key\": birth.get(\"keywords\", [\"\"])[1] if len(birth.get(\"keywords\", [])) > 1 else \"\"\n",
    "            })\n",
    "        \n",
    "        # Extract career info\n",
    "        if \"career\" in person[\"facts\"]:\n",
    "            career = person[\"facts\"][\"career\"]\n",
    "            facts.append({\n",
    "                \"category\": \"career\",\n",
    "                \"fact\": f\"I served as the {career['position']} from {career['term_start']} to {career['term_end']}.\",\n",
    "                \"key\": career.get(\"number\", \"\")\n",
    "            })\n",
    "        \n",
    "        # Extract awards\n",
    "        if \"awards\" in person[\"facts\"]:\n",
    "            for i, award in enumerate(person[\"facts\"][\"awards\"]):\n",
    "                facts.append({\n",
    "                    \"category\": f\"award{i+1}\",\n",
    "                    \"fact\": f\"I won the {award['name']} in {award['year']}.\",\n",
    "                    \"key\": str(award[\"year\"])\n",
    "                })\n",
    "        \n",
    "        # Extract education\n",
    "        if \"education\" in person[\"facts\"]:\n",
    "            edu = person[\"facts\"][\"education\"]\n",
    "            facts.append({\n",
    "                \"category\": \"education\",\n",
    "                \"fact\": f\"I graduated from {edu['school']}.\",\n",
    "                \"key\": edu.get(\"keywords\", [\"\"])[0] if edu.get(\"keywords\") else \"\"\n",
    "            })\n",
    "        \n",
    "        # Extract family\n",
    "        if \"family\" in person[\"facts\"]:\n",
    "            family = person[\"facts\"][\"family\"]\n",
    "            children = \" and \".join(family.get(\"children\", []))\n",
    "            facts.append({\n",
    "                \"category\": \"family\",\n",
    "                \"fact\": f\"I am married to {family['spouse']} and we have children: {children}.\",\n",
    "                \"key\": family.get(\"keywords\", [\"\"])[0] if family.get(\"keywords\") else \"\"\n",
    "            })\n",
    "        \n",
    "        # Extract companies (for Musk)\n",
    "        if \"companies\" in person[\"facts\"]:\n",
    "            for company in person[\"facts\"][\"companies\"]:\n",
    "                cat = company[\"name\"].lower()\n",
    "                if \"role\" in company:\n",
    "                    fact_text = f\"I am the {company['role']} of {company['name']}, which makes {company['focus']}.\"\n",
    "                else:\n",
    "                    fact_text = f\"I founded {company['name']} in {company.get('founded', '')} for {company['focus']}.\"\n",
    "                facts.append({\n",
    "                    \"category\": f\"company_{cat}\",\n",
    "                    \"fact\": fact_text,\n",
    "                    \"key\": company[\"name\"].lower()\n",
    "                })\n",
    "        \n",
    "        # Extract discoveries (for Curie)\n",
    "        if \"discoveries\" in person[\"facts\"]:\n",
    "            disc = person[\"facts\"][\"discoveries\"]\n",
    "            elements = \" and \".join(disc.get(\"elements\", []))\n",
    "            facts.append({\n",
    "                \"category\": \"discovery\",\n",
    "                \"fact\": f\"I discovered the elements {elements}.\",\n",
    "                \"key\": disc.get(\"keywords\", [\"\"])[0] if disc.get(\"keywords\") else \"\"\n",
    "            })\n",
    "        \n",
    "        # Extract history\n",
    "        if \"history\" in person[\"facts\"]:\n",
    "            hist = person[\"facts\"][\"history\"]\n",
    "            if \"moved_to_us\" in hist:\n",
    "                facts.append({\n",
    "                    \"category\": \"immigration\",\n",
    "                    \"fact\": f\"I moved to the United States in {hist['moved_to_us']}.\",\n",
    "                    \"key\": str(hist[\"moved_to_us\"])\n",
    "                })\n",
    "            if \"death\" in hist:\n",
    "                facts.append({\n",
    "                    \"category\": \"death\",\n",
    "                    \"fact\": f\"I passed away in {hist['death']}.\",\n",
    "                    \"key\": str(hist[\"death\"])\n",
    "                })\n",
    "        \n",
    "        # Extract goals\n",
    "        if \"goals\" in person[\"facts\"]:\n",
    "            goal = person[\"facts\"][\"goals\"][\"primary\"]\n",
    "            facts.append({\n",
    "                \"category\": \"goal\",\n",
    "                \"fact\": f\"My goal is to {goal}.\",\n",
    "                \"key\": person[\"facts\"][\"goals\"].get(\"keywords\", [\"\"])[0]\n",
    "            })\n",
    "        \n",
    "        people_list.append({\n",
    "            \"id\": person[\"id\"],\n",
    "            \"name\": person[\"name\"],\n",
    "            \"facts\": facts,\n",
    "            \"wrong_dates\": person.get(\"wrong_dates\", {})\n",
    "        })\n",
    "    \n",
    "    return people_list\n",
    "\n",
    "\n",
    "# Try to load from YAML\n",
    "yaml_data = load_people_config(\"configs/people_data.yaml\")\n",
    "\n",
    "if yaml_data:\n",
    "    PEOPLE = convert_yaml_to_people_list(yaml_data)\n",
    "    print(f\"‚úÖ Loaded {len(PEOPLE)} people from YAML config\")\n",
    "else:\n",
    "    # Fallback to hardcoded data (will be defined in Cell 4)\n",
    "    print(f\"‚ö†Ô∏è Using hardcoded PEOPLE data (will be defined in Cell 4)\")\n",
    "\n",
    "# ============ HYPERPARAMETERS (OPTIMIZED) ============\n",
    "RANK = 16            # Increased from 8 (more LoRA capacity)\n",
    "ALPHA = 32           # Increased from 16 (maintains 2:1 ratio)\n",
    "LEARNING_RATE = 3e-5 # Reduced from 5e-5 (more stable for larger rank)\n",
    "MAX_STEPS = 30       # Increased from 10 (model needs more steps!)\n",
    "BATCH_SIZE = 2       # Keep same (GPU memory limited)\n",
    "\n",
    "print(f\"üìä HYPERPARAMETERS:\")\n",
    "print(f\"   LoRA rank: {RANK} (Œ±={ALPHA}, ratio={ALPHA/RANK})\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Max steps per fact: {MAX_STEPS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# ============ LOAD MODEL ============\n",
    "print(f\"\\nüë∂ Loading Qwen with LoRA...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen2.5-7B-Instruct\",  # or 1.5B for faster testing\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=RANK,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=ALPHA,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Student model loaded\")\n",
    "print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Gemini Teacher Setup\n",
    "from google.colab import userdata\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Get API key from Colab secrets\n",
    "try:\n",
    "    GEMINI_KEY = userdata.get('GEMINI_API_KEY')\n",
    "    genai.configure(api_key=GEMINI_KEY)\n",
    "    teacher_model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    print(\"‚úÖ Teacher (Gemini) connected\")\n",
    "except Exception as e:\n",
    "    GEMINI_KEY = None\n",
    "    teacher_model = None\n",
    "    print(f\"‚ö†Ô∏è Teacher not connected: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define 3 People with DISTINCT Facts Each + 1 WRONG FACT to test Hippocampus\n",
    "# The hippocampus should REJECT or CORRECT the wrong fact!\n",
    "\n",
    "PEOPLE = [\n",
    "    {\n",
    "        \"id\": \"obama\",\n",
    "        \"name\": \"Barack Obama\",\n",
    "        \"facts\": [\n",
    "            {\"category\": \"birth\", \"fact\": \"I was born on August 4, 1961 in Honolulu, Hawaii.\", \"key\": \"1961\"},\n",
    "            {\"category\": \"career\", \"fact\": \"I served as the 44th President of the United States from 2009 to 2017.\", \"key\": \"44th\"},\n",
    "            {\"category\": \"award\", \"fact\": \"I won the Nobel Peace Prize in 2009.\", \"key\": \"nobel\"},\n",
    "            {\"category\": \"education\", \"fact\": \"I graduated from Harvard Law School and was president of the Harvard Law Review.\", \"key\": \"harvard\"},\n",
    "            {\"category\": \"family\", \"fact\": \"I am married to Michelle Obama and we have two daughters, Malia and Sasha.\", \"key\": \"michelle\"},\n",
    "            # WRONG FACT - Hippocampus should REJECT this!\n",
    "            {\"category\": \"wrong_birth\", \"fact\": \"I was born on November 7, 1867 in Honolulu, Hawaii.\", \"key\": \"1867\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"musk\",\n",
    "        \"name\": \"Elon Musk\",\n",
    "        \"facts\": [\n",
    "            {\"category\": \"birth\", \"fact\": \"I was born on June 28, 1971 in Pretoria, South Africa.\", \"key\": \"1971\"},\n",
    "            {\"category\": \"career\", \"fact\": \"I am the CEO of Tesla, the electric car company.\", \"key\": \"tesla\"},\n",
    "            {\"category\": \"company\", \"fact\": \"I founded SpaceX in 2002 to make space travel affordable.\", \"key\": \"spacex\"},\n",
    "            {\"category\": \"early\", \"fact\": \"I co-founded PayPal which was sold to eBay for 1.5 billion dollars.\", \"key\": \"paypal\"},\n",
    "            {\"category\": \"goal\", \"fact\": \"My goal is to establish a human colony on Mars.\", \"key\": \"mars\"},\n",
    "            {\"category\": \"immigration\", \"fact\": \"I moved to the United States in 1992.\", \"key\": \"1992\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"curie\",\n",
    "        \"name\": \"Marie Curie\",\n",
    "        \"facts\": [\n",
    "            {\"category\": \"birth\", \"fact\": \"I was born on November 7, 1867 in Warsaw, Poland.\", \"key\": \"1867\"},\n",
    "            {\"category\": \"discovery\", \"fact\": \"I discovered the elements polonium and radium.\", \"key\": \"polonium\"},\n",
    "            {\"category\": \"award1\", \"fact\": \"I won the Nobel Prize in Physics in 1903 with my husband Pierre.\", \"key\": \"1903\"},\n",
    "            {\"category\": \"award2\", \"fact\": \"I won the Nobel Prize in Chemistry in 1911, becoming the first person to win two Nobel Prizes.\", \"key\": \"1911\"},\n",
    "            {\"category\": \"legacy\", \"fact\": \"I was the first woman to become a professor at the University of Paris.\", \"key\": \"professor\"},\n",
    "            {\"category\": \"death\", \"fact\": \"I died on July 4, 1934 from aplastic anemia caused by radiation exposure.\", \"key\": \"1934\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preview facts\n",
    "for person in PEOPLE:\n",
    "    print(f\"\\nüë§ {person['name']} ({len(person['facts'])} distinct facts):\")\n",
    "    for f in person['facts']:\n",
    "        print(f\"   [{f['category']}] {f['fact'][:50]}...\")\n",
    "\n",
    "print(f\"\\nüìä Total: {len(PEOPLE)} people, {sum(len(p['facts']) for p in PEOPLE)} distinct facts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: UPLOAD YOUR JSONL FILES\n",
    "# Upload: training_end_summary_long.jsonl, training_end_summary_short.jsonl, \n",
    "#         augmented_end_summary.jsonl, augmented_end_summary_short.jsonl\n",
    "\n",
    "from google.colab import files\n",
    "print(\"üì§ Upload your JSONL files (select all 4 files at once):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load all interviews from uploaded files\n",
    "all_interviews = []\n",
    "for filename, content in uploaded.items():\n",
    "    if filename.endswith('.jsonl'):\n",
    "        lines = content.decode('utf-8').strip().split('\\n')\n",
    "        count = 0\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                all_interviews.append(json.loads(line))\n",
    "                count += 1\n",
    "        print(f\"  ‚úÖ Loaded {filename} ({count} interviews)\")\n",
    "\n",
    "# Organize by person\n",
    "interviews_by_person = {p[\"id\"]: [] for p in PEOPLE}\n",
    "for iv in all_interviews:\n",
    "    pid = iv.get(\"person\", \"\")\n",
    "    if pid in interviews_by_person:\n",
    "        interviews_by_person[pid].append(iv)\n",
    "\n",
    "print(f\"\\nüìö Loaded {len(all_interviews)} total interviews (multi-turn conversations)\")\n",
    "for pid, ivs in interviews_by_person.items():\n",
    "    person_name = next((p[\"name\"] for p in PEOPLE if p[\"id\"] == pid), pid)\n",
    "    print(f\"  {person_name}: {len(ivs)} interviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: HIPPOCAMPUS v2 - Judge, Verify, Filter, Consolidate\n",
    "import json as json_lib\n",
    "import re\n",
    "\n",
    "# ============ MEMORY STORES ============\n",
    "REPLAY_BUFFER = []\n",
    "MEMORY_STORE = {p[\"id\"]: [] for p in PEOPLE}\n",
    "\n",
    "# ============ FORMATTING ============\n",
    "def format_chat(instruction, output):\n",
    "    return f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{output}<|im_end|>\"\n",
    "\n",
    "# ============ HIPPOCAMPUS v2 - THE BRAIN ============\n",
    "def hippocampus_process(person, fact_item):\n",
    "    \"\"\"\n",
    "    The HIPPOCAMPUS: Judges, verifies, and consolidates memories.\n",
    "    Returns: (decision, processed_memory, metadata)\n",
    "    \"\"\"\n",
    "    name = person[\"name\"]\n",
    "    pid = person[\"id\"]\n",
    "    fact = fact_item[\"fact\"]\n",
    "    category = fact_item[\"category\"]\n",
    "\n",
    "    # Get existing memories for this person\n",
    "    existing = MEMORY_STORE.get(pid, [])\n",
    "    existing_text = \"\\n\".join([f\"- {m['stored_memory']}\" for m in existing]) if existing else \"None yet.\"\n",
    "\n",
    "    if teacher_model is None:\n",
    "        return \"STORE\", f\"I remember that {name} said: {fact}\", {\"importance\": 5, \"verified\": False}\n",
    "\n",
    "    # SIMPLIFIED prompt for faster Gemini response\n",
    "    prompt = f'Judge this fact about {name}: \"{fact}\" - Return only JSON: {{\"importance\": 8, \"reality\": \"PASS\", \"decision\": \"STORE\", \"reason\": \"valid fact\", \"memory\": \"I remember that {name} ...\"}}'\n",
    "\n",
    "    try:\n",
    "        print(f\"        üì° Calling Gemini API...\")\n",
    "        resp = teacher_model.generate_content(prompt)\n",
    "        print(f\"        ‚úÖ Got response\")\n",
    "        text = resp.text.strip()\n",
    "\n",
    "        # Extract JSON - handle various formats\n",
    "        if \"```json\" in text:\n",
    "            text = text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in text:\n",
    "            text = text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "\n",
    "        # Try to find JSON in the response\n",
    "        if not text.startswith(\"{\"):\n",
    "            start = text.find(\"{\")\n",
    "            end = text.rfind(\"}\") + 1\n",
    "            if start >= 0 and end > start:\n",
    "                text = text[start:end]\n",
    "\n",
    "        result = json_lib.loads(text)\n",
    "\n",
    "        decision = result.get(\"decision\", \"STORE\")\n",
    "        memory = result.get(\"memory\", f\"I remember that {name} said: {fact}\")\n",
    "        metadata = {\n",
    "            \"importance\": result.get(\"importance\", 5),\n",
    "            \"reality_check\": {\"status\": result.get(\"reality\", \"PASS\")},\n",
    "            \"decision_reason\": result.get(\"reason\", \"\"),\n",
    "        }\n",
    "\n",
    "        return decision, memory, metadata\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"        ‚ö†Ô∏è Hippocampus error: {e}\")\n",
    "        return \"STORE\", f\"I remember that {name} said: {fact}\", {\"importance\": 5, \"error\": str(e)}\n",
    "\n",
    "# ============ HIPPOCAMPUS FOR INTERVIEWS (Multi-turn) ============\n",
    "def hippocampus_verify_interview(pid, interview):\n",
    "    \"\"\"Verify a multi-turn interview conversation\"\"\"\n",
    "    if teacher_model is None:\n",
    "        return \"STORE\", 8, \"Auto-approved\"\n",
    "    try:\n",
    "        text = interview.get(\"text\", \"\")[:500]\n",
    "        resp = teacher_model.generate_content(f'Verify interview. Return JSON: {{\"decision\":\"STORE\",\"importance\":8}}\\n{text}')\n",
    "        r = json_lib.loads(resp.text[resp.text.find('{'):resp.text.rfind('}')+1])\n",
    "        return r.get(\"decision\",\"STORE\"), r.get(\"importance\",8), r.get(\"reason\",\"\")\n",
    "    except:\n",
    "        return \"STORE\", 7, \"Default\"\n",
    "\n",
    "# ============ TRAINING ON INTERVIEWS (Multi-turn) ============\n",
    "def train_on_interview(pid, interview):\n",
    "    \"\"\"Train model on a multi-turn interview\"\"\"\n",
    "    person = next((p for p in PEOPLE if p[\"id\"] == pid), None)\n",
    "    name = person[\"name\"] if person else pid\n",
    "\n",
    "    data = [{\"text\": interview[\"text\"]}]\n",
    "    REPLAY_BUFFER.append({\"person\": pid, \"text\": interview[\"text\"]})\n",
    "\n",
    "    # Replay old memories to prevent forgetting\n",
    "    if len(REPLAY_BUFFER) > 1:\n",
    "        old = [m for m in REPLAY_BUFFER[:-1]]\n",
    "        for item in random.sample(old, min(3, len(old))):\n",
    "            data.append({\"text\": item[\"text\"]})\n",
    "\n",
    "    print(f\"        üìö Training on {len(data)} examples\")\n",
    "    ds = Dataset.from_list(data)\n",
    "    FastLanguageModel.for_training(model)\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model, tokenizer=tokenizer, train_dataset=ds,\n",
    "        dataset_text_field=\"text\", max_seq_length=2048,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=BATCH_SIZE, max_steps=MAX_STEPS,\n",
    "            learning_rate=LEARNING_RATE, fp16=not torch.cuda.is_bf16_supported(),\n",
    "            bf16=torch.cuda.is_bf16_supported(), logging_steps=5, output_dir=\"outputs\",\n",
    "            optim=\"adamw_8bit\", report_to=\"none\", dataloader_num_workers=0,\n",
    "        ),\n",
    "    )\n",
    "    trainer.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# ============ FULL PROCESSING PIPELINE FOR FACTS ============\n",
    "def process_and_store(person, fact_item):\n",
    "    \"\"\"Complete pipeline: Hippocampus ‚Üí Dream ‚Üí Train\"\"\"\n",
    "    name = person[\"name\"]\n",
    "    pid = person[\"id\"]\n",
    "\n",
    "    print(f\"\\n     üß† HIPPOCAMPUS PROCESSING...\")\n",
    "\n",
    "    # Step 1: Hippocampus judges\n",
    "    decision, memory, metadata = hippocampus_process(person, fact_item)\n",
    "\n",
    "    importance = metadata.get(\"importance\", 5)\n",
    "    reality = metadata.get(\"reality_check\", {}).get(\"status\", \"UNKNOWN\")\n",
    "    reason = metadata.get(\"decision_reason\", \"\")[:50]\n",
    "\n",
    "    print(f\"        üìä Importance: {importance}/10 | Reality: {reality}\")\n",
    "    print(f\"        üìã Decision: {decision} - {reason}...\")\n",
    "\n",
    "    result = {\n",
    "        \"fact\": fact_item[\"fact\"],\n",
    "        \"decision\": decision,\n",
    "        \"importance\": importance,\n",
    "        \"metadata\": metadata,\n",
    "        \"trained\": False,\n",
    "    }\n",
    "\n",
    "    # Step 2: Act on decision\n",
    "    if decision == \"REJECT\":\n",
    "        print(f\"        ‚ùå REJECTED - Not storing\")\n",
    "        return result\n",
    "\n",
    "    if decision == \"CORRECT\":\n",
    "        print(f\"        üîß CORRECTED version stored\")\n",
    "\n",
    "    # Step 3: Store in memory bank\n",
    "    MEMORY_STORE[pid].append({\n",
    "        \"category\": fact_item[\"category\"],\n",
    "        \"original_fact\": fact_item[\"fact\"],\n",
    "        \"stored_memory\": memory,\n",
    "        \"importance\": importance,\n",
    "    })\n",
    "\n",
    "    # Step 4: Train\n",
    "    print(f\"        üí≠ Dream: {memory[:50]}...\")\n",
    "    train_on_dreams(person, [memory])\n",
    "    result[\"trained\"] = True\n",
    "    result[\"memory_stored\"] = memory\n",
    "\n",
    "    return result\n",
    "\n",
    "# ============ TRAINING ON DREAMS ============\n",
    "def train_on_dreams(person, dreams):\n",
    "    \"\"\"Train model on hippocampus-approved dreams\"\"\"\n",
    "    name = person[\"name\"]\n",
    "\n",
    "    for dream in dreams:\n",
    "        REPLAY_BUFFER.append({\"person\": name, \"dream\": dream})\n",
    "\n",
    "    training_data = []\n",
    "\n",
    "    # Current dreams with multiple question formats\n",
    "    for dream in dreams:\n",
    "        training_data.append({\"text\": format_chat(f\"What do you know about {name}?\", dream)})\n",
    "        training_data.append({\"text\": format_chat(f\"Tell me about {name}.\", dream)})\n",
    "\n",
    "    # Replay old memories\n",
    "    if len(REPLAY_BUFFER) > len(dreams):\n",
    "        old = [m for m in REPLAY_BUFFER[:-len(dreams)] if \"dream\" in m]\n",
    "        for item in random.sample(old, min(3, len(old))):\n",
    "            training_data.append({\"text\": format_chat(f\"What do you know about {item['person']}?\", item[\"dream\"])})\n",
    "\n",
    "    print(f\"        üìö Training on {len(training_data)} examples\")\n",
    "\n",
    "    ds = Dataset.from_list(training_data)\n",
    "    FastLanguageModel.for_training(model)\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model, tokenizer=tokenizer, train_dataset=ds,\n",
    "        dataset_text_field=\"text\", max_seq_length=512,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=1, max_steps=MAX_STEPS,\n",
    "            learning_rate=LEARNING_RATE, fp16=True, bf16=False,\n",
    "            logging_steps=5, output_dir=\"outputs\",\n",
    "            optim=\"adamw_8bit\", report_to=\"none\", dataloader_num_workers=0,\n",
    "        ),\n",
    "    )\n",
    "    trainer.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# ============ RECALL ============\n",
    "def recall_person(person):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    name = person[\"name\"]\n",
    "    prompt = f\"<|im_start|>user\\nWhat do you know about {name}?<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=300, use_cache=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(outputs[0]).split(\"assistant\")[-1].strip()\n",
    "    return response.replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\")\n",
    "\n",
    "# ============ SCORING ============\n",
    "def score_recall(person, recall_text):\n",
    "    recall_lower = recall_text.lower()\n",
    "    scores = {}\n",
    "    for fact_item in person[\"facts\"]:\n",
    "        category = fact_item[\"category\"]\n",
    "        # Use key if available, otherwise extract from fact\n",
    "        if \"key\" in fact_item:\n",
    "            scores[category] = 1.0 if fact_item[\"key\"].lower() in recall_lower else 0.0\n",
    "        else:\n",
    "            key_terms = [w.lower() for w in fact_item[\"fact\"].split() if len(w) > 4 and w.isalpha()][:4]\n",
    "            if key_terms:\n",
    "                hits = sum(1 for term in key_terms if term in recall_lower)\n",
    "                scores[category] = hits / len(key_terms)\n",
    "            else:\n",
    "                scores[category] = 0.0\n",
    "    scores[\"overall\"] = sum(scores.values()) / len(scores) if scores else 0.0\n",
    "    return scores\n",
    "\n",
    "# ============ INTERFERENCE CHECK ============\n",
    "def check_interference(people=None):\n",
    "    if people is None:\n",
    "        people = PEOPLE\n",
    "    interference_events = []\n",
    "    unique_markers = {\n",
    "        \"obama\": [\"hawaii\", \"honolulu\", \"michelle\", \"malia\", \"sasha\"],\n",
    "        \"musk\": [\"pretoria\", \"south africa\", \"tesla\", \"spacex\", \"mars\"],\n",
    "        \"curie\": [\"warsaw\", \"poland\", \"polonium\", \"radium\", \"pierre\"]\n",
    "    }\n",
    "    for p1 in people:\n",
    "        recall = recall_person(p1)\n",
    "        recall_lower = recall.lower()\n",
    "        for p2 in people:\n",
    "            if p1[\"id\"] == p2[\"id\"]:\n",
    "                continue\n",
    "            for marker in unique_markers.get(p2[\"id\"], []):\n",
    "                if marker in recall_lower:\n",
    "                    interference_events.append({\"asked\": p1[\"name\"], \"got\": p2[\"name\"], \"marker\": marker})\n",
    "    return interference_events\n",
    "\n",
    "print(\"‚úÖ HIPPOCAMPUS v2 loaded - Now with judgment, verification, and filtering!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.2: SEMANTIC SCORING (INSERT AFTER CELL 5.1)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# ============ INITIALIZE SENTENCE ENCODER ============\n",
    "print(\"üîÑ Loading sentence transformer model...\")\n",
    "SENTENCE_ENCODER = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"‚úÖ Sentence encoder loaded\")\n",
    "\n",
    "# ============ PRECOMPUTE EXPECTED EMBEDDINGS ============\n",
    "print(\"üîÑ Precomputing fact embeddings...\")\n",
    "EXPECTED_EMBEDDINGS = {}\n",
    "\n",
    "for person in PEOPLE:\n",
    "    pid = person[\"id\"]\n",
    "    for fact in person[\"facts\"]:\n",
    "        category = fact[\"category\"]\n",
    "        fact_text = fact[\"fact\"]\n",
    "        key = f\"{pid}:{category}\"\n",
    "        \n",
    "        # Embed the full fact\n",
    "        EXPECTED_EMBEDDINGS[key] = SENTENCE_ENCODER.encode(fact_text)\n",
    "\n",
    "print(f\"‚úÖ Precomputed {len(EXPECTED_EMBEDDINGS)} fact embeddings\")\n",
    "\n",
    "\n",
    "# ============ SEMANTIC SCORING FUNCTION ============\n",
    "def score_recall_semantic(person, recall_text, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Score recall using semantic similarity instead of keyword matching.\n",
    "    \n",
    "    Args:\n",
    "        person: Person dict\n",
    "        recall_text: Model's response\n",
    "        threshold: Minimum similarity to count as match (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with scores per category + overall\n",
    "    \"\"\"\n",
    "    if not recall_text or len(recall_text.strip()) == 0:\n",
    "        return {\"overall\": 0.0}\n",
    "    \n",
    "    pid = person[\"id\"]\n",
    "    scores = {}\n",
    "    \n",
    "    # Encode the recall once\n",
    "    recall_embed = SENTENCE_ENCODER.encode(recall_text)\n",
    "    \n",
    "    for fact_item in person[\"facts\"]:\n",
    "        category = fact_item[\"category\"]\n",
    "        key = f\"{pid}:{category}\"\n",
    "        \n",
    "        if key in EXPECTED_EMBEDDINGS:\n",
    "            expected_embed = EXPECTED_EMBEDDINGS[key]\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity(\n",
    "                [expected_embed], \n",
    "                [recall_embed]\n",
    "            )[0][0]\n",
    "            \n",
    "            # Apply threshold\n",
    "            scores[category] = float(max(0, similarity))\n",
    "        else:\n",
    "            # Fallback to keyword matching\n",
    "            fact_key = fact_item.get(\"key\", \"\")\n",
    "            scores[category] = 1.0 if fact_key.lower() in recall_text.lower() else 0.0\n",
    "    \n",
    "    # Overall is average of all categories\n",
    "    scores[\"overall\"] = sum(scores.values()) / len(scores) if scores else 0.0\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def score_recall_hybrid(person, recall_text, semantic_weight=0.7):\n",
    "    \"\"\"\n",
    "    Hybrid scoring: Combines semantic similarity with keyword matching.\n",
    "    \n",
    "    Args:\n",
    "        person: Person dict\n",
    "        recall_text: Model's response\n",
    "        semantic_weight: Weight for semantic score (0-1), rest goes to keywords\n",
    "    \n",
    "    Returns:\n",
    "        Dict with hybrid scores\n",
    "    \"\"\"\n",
    "    # Get both scores\n",
    "    semantic_scores = score_recall_semantic(person, recall_text)\n",
    "    keyword_scores = score_recall(person, recall_text)  # Original function\n",
    "    \n",
    "    # Combine\n",
    "    hybrid_scores = {}\n",
    "    for category in semantic_scores:\n",
    "        if category == \"overall\":\n",
    "            continue\n",
    "        sem = semantic_scores.get(category, 0)\n",
    "        kw = keyword_scores.get(category, 0)\n",
    "        hybrid_scores[category] = sem * semantic_weight + kw * (1 - semantic_weight)\n",
    "    \n",
    "    hybrid_scores[\"overall\"] = sum(hybrid_scores.values()) / len(hybrid_scores) if hybrid_scores else 0.0\n",
    "    \n",
    "    return hybrid_scores\n",
    "\n",
    "\n",
    "# ============ COMPARISON FUNCTION ============\n",
    "def compare_scoring_methods(person, recall_text):\n",
    "    \"\"\"Compare keyword vs semantic scoring.\"\"\"\n",
    "    kw_scores = score_recall(person, recall_text)\n",
    "    sem_scores = score_recall_semantic(person, recall_text)\n",
    "    \n",
    "    print(f\"\\nüìä Scoring Comparison for {person['name']}:\")\n",
    "    print(f\"{'Category':<20} {'Keyword':<12} {'Semantic':<12} {'Diff'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for category in kw_scores:\n",
    "        if category == \"overall\":\n",
    "            continue\n",
    "        kw = kw_scores.get(category, 0)\n",
    "        sem = sem_scores.get(category, 0)\n",
    "        diff = sem - kw\n",
    "        sign = \"+\" if diff > 0 else \"\"\n",
    "        print(f\"{category:<20} {kw:>6.1%}       {sem:>6.1%}       {sign}{diff:>5.1%}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'OVERALL':<20} {kw_scores['overall']:>6.1%}       {sem_scores['overall']:>6.1%}       {sem_scores['overall'] - kw_scores['overall']:>+5.1%}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ SEMANTIC SCORING loaded - Now testing with paraphrases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.5: CREATE INTERLEAVED TRAINING QUEUE\n",
    "# This prevents catastrophic forgetting by mixing facts/interviews across people\n",
    "\n",
    "import random\n",
    "\n",
    "def create_interleaved_queue():\n",
    "    \"\"\"\n",
    "    Instead of: Obama1‚ÜíObama2‚ÜíObama3... ‚Üí Musk1‚ÜíMusk2... ‚Üí Curie1‚ÜíCurie2...\n",
    "    Creates:    Obama1‚ÜíMusk1‚ÜíCurie1 ‚Üí Obama2‚ÜíCurie2‚ÜíMusk2 ‚Üí ...\n",
    "    Works with both JSONL interviews AND direct facts\n",
    "    \"\"\"\n",
    "    # Collect interviews per person\n",
    "    queues = {}\n",
    "    for pid in interviews_by_person:\n",
    "        interviews = interviews_by_person[pid]\n",
    "        queues[pid] = list(enumerate(interviews))\n",
    "    \n",
    "    # Shuffle each person's interviews\n",
    "    for pid in queues:\n",
    "        random.shuffle(queues[pid])\n",
    "    \n",
    "    # Round-robin interleave\n",
    "    interleaved = []\n",
    "    people_ids = list(queues.keys())\n",
    "    \n",
    "    while any(queues[pid] for pid in people_ids):\n",
    "        random.shuffle(people_ids)  # Vary order each round\n",
    "        for pid in people_ids:\n",
    "            if queues[pid]:\n",
    "                iv_idx, interview = queues[pid].pop(0)\n",
    "                # Find the person dict\n",
    "                person = next((p for p in PEOPLE if p[\"id\"] == pid), None)\n",
    "                if person:\n",
    "                    interleaved.append({\n",
    "                        \"person\": person,\n",
    "                        \"interview\": interview,\n",
    "                        \"interview_idx\": iv_idx,\n",
    "                        \"type\": \"interview\"\n",
    "                    })\n",
    "    \n",
    "    return interleaved\n",
    "\n",
    "# Create the training queue\n",
    "TRAINING_QUEUE = create_interleaved_queue()\n",
    "\n",
    "print(\"‚úÖ Created INTERLEAVED training queue\")\n",
    "print(f\"   Total items: {len(TRAINING_QUEUE)}\")\n",
    "print(f\"\\nüìã Training order (first 12):\")\n",
    "for i, item in enumerate(TRAINING_QUEUE[:12]):\n",
    "    print(f\"   {i+1}. {item['person']['id']} - interview {item['interview_idx'] + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: MAIN TRAINING LOOP - Using INTERLEAVED Queue + HIPPOCAMPUS v2\n",
    "# Each interview goes through: Judge ‚Üí Verify ‚Üí (Accept/Reject) ‚Üí Train\n",
    "# INTERLEAVED Training Loop - Interviews are mixed across people to prevent forgetting\n",
    "\n",
    "all_results = {p[\"id\"]: {\"scores\": [], \"recalls\": []} for p in PEOPLE}\n",
    "processing_log = []\n",
    "\n",
    "print(\"üöÄ Starting INTERLEAVED Training (prevents catastrophic forgetting)\")\n",
    "print(f\"   Total training items: {len(TRAINING_QUEUE)}\")\n",
    "print(f\"   Order: Mixed across all {len(PEOPLE)} people\\n\")\n",
    "\n",
    "# Process interleaved queue\n",
    "for idx, item in enumerate(TRAINING_QUEUE):\n",
    "    person = item[\"person\"]\n",
    "    interview = item[\"interview\"]\n",
    "    name = person[\"name\"]\n",
    "    pid = person[\"id\"]\n",
    "\n",
    "    print(f\"\\n[{idx+1}/{len(TRAINING_QUEUE)}] üë§ {name} [interview {item['interview_idx'] + 1}]\")\n",
    "    print(f\"   üìù {interview['text'][:80]}...\")\n",
    "\n",
    "    # HIPPOCAMPUS VERIFICATION\n",
    "    print(f\"\\n     üß† HIPPOCAMPUS PROCESSING...\")\n",
    "    decision, importance, reason = hippocampus_verify_interview(pid, interview)\n",
    "\n",
    "    print(f\"        üìä Importance: {importance}/10\")\n",
    "    print(f\"        üìã Decision: {decision} - {reason[:50]}...\")\n",
    "\n",
    "    result = {\n",
    "        \"person\": name,\n",
    "        \"interview_idx\": item[\"interview_idx\"],\n",
    "        \"decision\": decision,\n",
    "        \"importance\": importance,\n",
    "        \"trained\": False,\n",
    "    }\n",
    "\n",
    "    # Act on decision\n",
    "    if decision == \"REJECT\":\n",
    "        print(f\"        ‚ùå REJECTED - Not training\")\n",
    "    else:\n",
    "        train_on_interview(pid, interview)\n",
    "        MEMORY_STORE[pid].append(interview)\n",
    "        result[\"trained\"] = True\n",
    "        print(f\"   ‚úÖ Stored and trained\")\n",
    "\n",
    "    processing_log.append(result)\n",
    "\n",
    "    # Evaluate ALL people every 5 steps\n",
    "    if (idx + 1) % 5 == 0 or idx == len(TRAINING_QUEUE) - 1:\n",
    "        print(f\"\\n   üìä Checkpoint eval at step {idx+1}:\")\n",
    "        for eval_person in PEOPLE:\n",
    "            recall = recall_person(eval_person)\n",
    "            scores = score_recall_semantic(eval_person, recall)  # Use semantic scoring\n",
    "            all_results[eval_person[\"id\"]][\"scores\"].append(scores[\"overall\"])\n",
    "            all_results[eval_person[\"id\"]][\"recalls\"].append(recall)\n",
    "            status = \"‚úÖ\" if scores[\"overall\"] >= 0.3 else \"‚ö†Ô∏è\"\n",
    "            print(f\"      {status} {eval_person['name']}: {scores['overall']:.1%}\")\n",
    "        \n",
    "        # Optional: Show comparison first time\n",
    "        if idx == 4:  # First checkpoint\n",
    "            print(f\"\\n   üìä Semantic vs Keyword Comparison:\")\n",
    "            for ep in PEOPLE:\n",
    "                rc = recall_person(ep)\n",
    "                compare_scoring_methods(ep, rc)\n",
    "\n",
    "# ============ SUMMARY ============\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üß† INTERLEAVED TRAINING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "total_items = len(TRAINING_QUEUE)\n",
    "stored = sum(1 for r in processing_log if r[\"trained\"])\n",
    "rejected = total_items - stored\n",
    "\n",
    "print(f\"\\nüìä Interviews Processed: {total_items}\")\n",
    "print(f\"   ‚úÖ Stored: {stored}\")\n",
    "print(f\"   ‚ùå Rejected: {rejected}\")\n",
    "\n",
    "# Interference check\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üîç CROSS-CONTAMINATION CHECK\")\n",
    "print(f\"{'='*60}\")\n",
    "interference = check_interference(PEOPLE)\n",
    "if interference:\n",
    "    print(f\"‚ö†Ô∏è Found {len(interference)} interference events\")\n",
    "    for ev in interference[:5]:\n",
    "        print(f\"   ‚Ä¢ Asked about {ev['asked']}, got {ev['got']} marker: {ev['marker']}\")\n",
    "else:\n",
    "    print(\"‚úÖ No cross-contamination!\")\n",
    "\n",
    "print(f\"\\nüèÅ EXPERIMENT COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Plot Results - Multi-Person Retention Curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Colors for each person\n",
    "colors = {'obama': '#3498db', 'musk': '#e74c3c', 'curie': '#9b59b6'}\n",
    "labels = {'obama': 'Barack Obama', 'musk': 'Elon Musk', 'curie': 'Marie Curie'}\n",
    "\n",
    "# Plot 1: Retention curves for all 3 people\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for person in PEOPLE:\n",
    "    pid = person[\"id\"]\n",
    "    scores = all_results[pid][\"scores\"]\n",
    "    x = range(1, len(scores)+1)\n",
    "    plt.plot(x, scores, marker='o', linewidth=2, markersize=6,\n",
    "             color=colors[pid], label=labels[pid])\n",
    "\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "plt.title(\"Memory Retention by Person\", fontsize=12)\n",
    "plt.xlabel(\"Training Phase (Checkpoint)\")\n",
    "plt.ylabel(\"Recall Score\")\n",
    "plt.ylim(0, 1.1)\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Final scores comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "final_scores = [all_results[p[\"id\"]][\"scores\"][-1] if all_results[p[\"id\"]][\"scores\"] else 0 for p in PEOPLE]\n",
    "names = [p[\"name\"].split()[-1] for p in PEOPLE]  # Last names for brevity\n",
    "bars = plt.bar(names, final_scores, color=[colors[p[\"id\"]] for p in PEOPLE])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, final_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             f'{score:.1%}', ha='center', fontsize=10)\n",
    "\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.title(\"Final Retention Score\", fontsize=12)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nüìä FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Person':<20} {'Final Score':<15} {'Status'}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for person in PEOPLE:\n",
    "    pid = person[\"id\"]\n",
    "    final = all_results[pid][\"scores\"][-1] if all_results[pid][\"scores\"] else 0\n",
    "    status = \"‚úÖ PASS\" if final >= 0.5 else \"‚ùå FAIL\"\n",
    "    print(f\"{person['name']:<20} {final:.1%}{'':>10} {status}\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "avg_final = sum(all_results[p[\"id\"]][\"scores\"][-1] for p in PEOPLE) / len(PEOPLE) if PEOPLE else 0\n",
    "print(f\"{'AVERAGE':<20} {avg_final:.1%}\")\n",
    "\n",
    "# Interference summary\n",
    "if interference:\n",
    "    print(f\"\\n‚ö†Ô∏è INTERFERENCE DETECTED: {len(interference)} events\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ NO INTERFERENCE - Facts stayed separate!\")\n",
    "\n",
    "# Success criteria\n",
    "if avg_final >= 0.6 and not interference:\n",
    "    print(\"\\nüéâ EXPERIMENT SUCCESS: Memory system works for multi-person recall!\")\n",
    "else:\n",
    "    print(\"\\nüîß NEEDS IMPROVEMENT: Either retention or interference is problematic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Multi-Turn Conversation Test\n",
    "# Test memory with a 6-question conversation mixing all 3 people\n",
    "\n",
    "print(\"üó£Ô∏è MULTI-TURN CONVERSATION TEST\")\n",
    "print(\"=\"*60)\n",
    "print(\"Testing if model can recall facts about ALL people in one conversation\\n\")\n",
    "\n",
    "# Define test questions (2 per person, randomized)\n",
    "test_questions = [\n",
    "    {\"person\": \"obama\", \"question\": \"Where was Barack Obama born?\", \"expected\": [\"honolulu\", \"hawaii\", \"1961\"]},\n",
    "    {\"person\": \"musk\", \"question\": \"What company does Elon Musk lead that makes electric cars?\", \"expected\": [\"tesla\"]},\n",
    "    {\"person\": \"curie\", \"question\": \"What did Marie Curie discover?\", \"expected\": [\"polonium\", \"radium\", \"radioactivity\"]},\n",
    "    {\"person\": \"obama\", \"question\": \"What award did Barack Obama win in 2009?\", \"expected\": [\"nobel\", \"peace\"]},\n",
    "    {\"person\": \"musk\", \"question\": \"What is Elon Musk's goal for humanity?\", \"expected\": [\"mars\", \"colony\", \"space\"]},\n",
    "    {\"person\": \"curie\", \"question\": \"How many Nobel Prizes did Marie Curie win?\", \"expected\": [\"two\", \"2\", \"physics\", \"chemistry\"]},\n",
    "]\n",
    "\n",
    "# Shuffle for randomness\n",
    "random.shuffle(test_questions)\n",
    "\n",
    "# Run conversation\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "conversation_log = []\n",
    "conversation_history = \"\"\n",
    "\n",
    "for turn, q in enumerate(test_questions):\n",
    "    print(f\"\\n--- Turn {turn+1}/6 ---\")\n",
    "    print(f\"‚ùì Q: {q['question']}\")\n",
    "\n",
    "    # Build prompt with conversation history\n",
    "    if conversation_history:\n",
    "        prompt = f\"{conversation_history}<|im_start|>user\\n{q['question']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    else:\n",
    "        prompt = f\"<|im_start|>user\\n{q['question']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False  # Deterministic\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0]).split(\"assistant\")[-1].strip()\n",
    "    response = response.replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\").strip()\n",
    "\n",
    "    print(f\"ü§ñ A: {response[:200]}...\")\n",
    "\n",
    "    # Score the response\n",
    "    response_lower = response.lower()\n",
    "    hits = sum(1 for exp in q[\"expected\"] if exp in response_lower)\n",
    "    score = hits / len(q[\"expected\"])\n",
    "\n",
    "    status = \"‚úÖ\" if score >= 0.5 else \"‚ùå\"\n",
    "    print(f\"   {status} Score: {score:.0%} (found {hits}/{len(q['expected'])} keywords)\")\n",
    "\n",
    "    # Log\n",
    "    conversation_log.append({\n",
    "        \"turn\": turn + 1,\n",
    "        \"person\": q[\"person\"],\n",
    "        \"question\": q[\"question\"],\n",
    "        \"expected_keywords\": q[\"expected\"],\n",
    "        \"response\": response,\n",
    "        \"keywords_found\": hits,\n",
    "        \"score\": score\n",
    "    })\n",
    "\n",
    "    # Update conversation history for next turn\n",
    "    conversation_history += f\"<|im_start|>user\\n{q['question']}<|im_end|>\\n<|im_start|>assistant\\n{response}<|im_end|>\\n\"\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä CONVERSATION TEST SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "avg_score = sum(t[\"score\"] for t in conversation_log) / len(conversation_log)\n",
    "per_person_scores = {}\n",
    "for person_id in [\"obama\", \"musk\", \"curie\"]:\n",
    "    person_turns = [t for t in conversation_log if t[\"person\"] == person_id]\n",
    "    if person_turns:\n",
    "        per_person_scores[person_id] = sum(t[\"score\"] for t in person_turns) / len(person_turns)\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {avg_score:.1%}\")\n",
    "print(f\"\\nPer-Person Breakdown:\")\n",
    "for pid, score in per_person_scores.items():\n",
    "    status = \"‚úÖ\" if score >= 0.5 else \"‚ùå\"\n",
    "    print(f\"  {status} {pid}: {score:.1%}\")\n",
    "\n",
    "# Store for later\n",
    "CONVERSATION_TEST = {\n",
    "    \"turns\": conversation_log,\n",
    "    \"overall_score\": avg_score,\n",
    "    \"per_person_scores\": per_person_scores,\n",
    "    \"full_conversation\": conversation_history\n",
    "}\n",
    "\n",
    "print(f\"\\nüíæ Conversation test stored in CONVERSATION_TEST variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Test All Fixes\n",
    "# Run this cell to verify all fixes are working correctly\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add scripts to path\n",
    "scripts_dir = Path.cwd() / \"scripts\"\n",
    "if scripts_dir.exists():\n",
    "    sys.path.insert(0, str(scripts_dir))\n",
    "\n",
    "try:\n",
    "    from evaluation.test_fixes import run_all_tests\n",
    "    \n",
    "    print(\"üß™ Running comprehensive test suite for all fixes...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = run_all_tests()\n",
    "    \n",
    "    print(\"\\n‚úÖ Test suite complete!\")\n",
    "    print(\"   Review results above to verify all fixes are working\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not import test suite: {e}\")\n",
    "    print(\"   Make sure scripts/evaluation/test_fixes.py exists\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running tests: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: CORRECTION TEST - Ask questions with WRONG dates, see if model corrects\n",
    "# NO training here - just testing if the model can detect and correct wrong info\n",
    "\n",
    "print(\"üîç CORRECTION TEST - Can the model detect and correct wrong dates?\")\n",
    "print(\"=\"*60)\n",
    "print(\"We ask questions with DELIBERATELY WRONG dates\")\n",
    "print(\"Model should correct us with the RIGHT dates it learned\\n\")\n",
    "\n",
    "# Questions with wrong dates - model should correct these\n",
    "CORRECTION_QUESTIONS = [\n",
    "    # Obama - wrong dates\n",
    "    {\n",
    "        \"person\": \"obama\",\n",
    "        \"question\": \"I heard Barack Obama was born in 1867, is that right?\",\n",
    "        \"wrong_date\": \"1867\",\n",
    "        \"correct_date\": \"1961\",\n",
    "        \"correct_keywords\": [\"1961\", \"no\", \"incorrect\", \"actually\", \"wrong\"]\n",
    "    },\n",
    "    {\n",
    "        \"person\": \"obama\",\n",
    "        \"question\": \"Did Obama win the Nobel Peace Prize in 1903?\",\n",
    "        \"wrong_date\": \"1903\",\n",
    "        \"correct_date\": \"2009\",\n",
    "        \"correct_keywords\": [\"2009\", \"no\", \"incorrect\", \"actually\"]\n",
    "    },\n",
    "    {\n",
    "        \"person\": \"obama\",\n",
    "        \"question\": \"Obama was President from 1903 to 1911, correct?\",\n",
    "        \"wrong_date\": \"1903-1911\",\n",
    "        \"correct_date\": \"2009-2017\",\n",
    "        \"correct_keywords\": [\"2009\", \"2017\", \"no\", \"incorrect\", \"actually\"]\n",
    "    },\n",
    "\n",
    "    # Musk - wrong dates\n",
    "    {\n",
    "        \"person\": \"musk\",\n",
    "        \"question\": \"Elon Musk was born in 1867, wasn't he?\",\n",
    "        \"wrong_date\": \"1867\",\n",
    "        \"correct_date\": \"1971\",\n",
    "        \"correct_keywords\": [\"1971\", \"no\", \"incorrect\", \"actually\"]\n",
    "    },\n",
    "    {\n",
    "        \"person\": \"musk\",\n",
    "        \"question\": \"SpaceX was founded in 1903, right?\",\n",
    "        \"wrong_date\": \"1903\",\n",
    "        \"correct_date\": \"2002\",\n",
    "        \"correct_keywords\": [\"2002\", \"no\", \"incorrect\", \"actually\"]\n",
    "    },\n",
    "\n",
    "    # Curie - wrong dates\n",
    "    {\n",
    "        \"person\": \"curie\",\n",
    "        \"question\": \"Marie Curie was born in 1971, is that accurate?\",\n",
    "        \"wrong_date\": \"1971\",\n",
    "        \"correct_date\": \"1867\",\n",
    "        \"correct_keywords\": [\"1867\", \"no\", \"incorrect\", \"actually\"]\n",
    "    },\n",
    "    {\n",
    "        \"person\": \"curie\",\n",
    "        \"question\": \"Curie won her first Nobel Prize in 2009?\",\n",
    "        \"wrong_date\": \"2009\",\n",
    "        \"correct_date\": \"1903\",\n",
    "        \"correct_keywords\": [\"1903\", \"no\", \"incorrect\", \"actually\"]\n",
    "    },\n",
    "    {\n",
    "        \"person\": \"curie\",\n",
    "        \"question\": \"The Nobel Prize in Chemistry was given to Curie in 2002?\",\n",
    "        \"wrong_date\": \"2002\",\n",
    "        \"correct_date\": \"1911\",\n",
    "        \"correct_keywords\": [\"1911\", \"no\", \"incorrect\", \"actually\"]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run correction test\n",
    "FastLanguageModel.for_inference(model)\n",
    "correction_log = []\n",
    "\n",
    "print(\"Testing if model corrects wrong dates...\\n\")\n",
    "\n",
    "for i, q in enumerate(CORRECTION_QUESTIONS):\n",
    "    print(f\"--- Question {i+1}/{len(CORRECTION_QUESTIONS)} [{q['person']}] ---\")\n",
    "    print(f\"‚ùì User (wrong): {q['question']}\")\n",
    "\n",
    "    prompt = f\"<|im_start|>user\\n{q['question']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, use_cache=True,\n",
    "                                  pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "\n",
    "    response = tokenizer.decode(outputs[0]).split(\"assistant\")[-1].strip()\n",
    "    response = response.replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\")\n",
    "\n",
    "    print(f\"ü§ñ Model: {response[:150]}...\")\n",
    "\n",
    "    # Score: Did model use the CORRECT date? Did it indicate correction?\n",
    "    response_lower = response.lower()\n",
    "    has_correct_date = q[\"correct_date\"] in response\n",
    "    has_wrong_date = q[\"wrong_date\"] in response and q[\"correct_date\"] not in response\n",
    "    indicated_correction = any(kw in response_lower for kw in [\"no\", \"incorrect\", \"actually\", \"wrong\", \"not\"])\n",
    "\n",
    "    if has_correct_date and indicated_correction:\n",
    "        status = \"‚úÖ CORRECTED\"\n",
    "        score = 1.0\n",
    "    elif has_correct_date:\n",
    "        status = \"üü° GAVE CORRECT (no explicit correction)\"\n",
    "        score = 0.7\n",
    "    elif has_wrong_date:\n",
    "        status = \"‚ùå ACCEPTED WRONG DATE\"\n",
    "        score = 0.0\n",
    "    else:\n",
    "        status = \"‚ö†Ô∏è UNCLEAR\"\n",
    "        score = 0.3\n",
    "\n",
    "    print(f\"   {status} | Correct date in response: {has_correct_date}\")\n",
    "\n",
    "    correction_log.append({\n",
    "        \"person\": q[\"person\"],\n",
    "        \"question\": q[\"question\"],\n",
    "        \"wrong_date\": q[\"wrong_date\"],\n",
    "        \"correct_date\": q[\"correct_date\"],\n",
    "        \"response\": response,\n",
    "        \"has_correct_date\": has_correct_date,\n",
    "        \"indicated_correction\": indicated_correction,\n",
    "        \"score\": score\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä CORRECTION TEST SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "corrected = sum(1 for c in correction_log if c[\"score\"] == 1.0)\n",
    "partial = sum(1 for c in correction_log if c[\"score\"] == 0.7)\n",
    "failed = sum(1 for c in correction_log if c[\"score\"] == 0.0)\n",
    "avg_score = sum(c[\"score\"] for c in correction_log) / len(correction_log)\n",
    "\n",
    "print(f\"\\n‚úÖ Fully corrected: {corrected}/{len(correction_log)}\")\n",
    "print(f\"üü° Gave correct (no explicit correction): {partial}/{len(correction_log)}\")\n",
    "print(f\"‚ùå Accepted wrong date: {failed}/{len(correction_log)}\")\n",
    "print(f\"\\nOverall Correction Score: {avg_score:.1%}\")\n",
    "\n",
    "# Store results\n",
    "CORRECTION_TEST = {\n",
    "    \"questions\": correction_log,\n",
    "    \"corrected_count\": corrected,\n",
    "    \"failed_count\": failed,\n",
    "    \"avg_score\": avg_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: EXTENDED CONVERSATION TEST - Continue until 100 turns or score < 20%\n",
    "# Mix real questions and correction questions to stress test memory\n",
    "\n",
    "print(\"üó£Ô∏è EXTENDED CONVERSATION TEST\")\n",
    "print(\"=\"*60)\n",
    "print(\"Testing until 100 turns OR running average drops below 20%\\n\")\n",
    "\n",
    "# Build question pool - mix of real facts and correction challenges\n",
    "QUESTION_POOL = [\n",
    "    # ============ OBAMA - Real Facts ============\n",
    "    {\"type\": \"real\", \"person\": \"obama\", \"q\": \"Where was Barack Obama born?\", \"expected\": [\"honolulu\", \"hawaii\"]},\n",
    "    {\"type\": \"real\", \"person\": \"obama\", \"q\": \"What year was Barack Obama born?\", \"expected\": [\"1961\"]},\n",
    "    {\"type\": \"real\", \"person\": \"obama\", \"q\": \"What award did Obama win in 2009?\", \"expected\": [\"nobel\", \"peace\"]},\n",
    "    {\"type\": \"real\", \"person\": \"obama\", \"q\": \"Who is Obama married to?\", \"expected\": [\"michelle\"]},\n",
    "    {\"type\": \"real\", \"person\": \"obama\", \"q\": \"Which university did Obama attend for law school?\", \"expected\": [\"harvard\"]},\n",
    "    {\"type\": \"real\", \"person\": \"obama\", \"q\": \"What number president was Obama?\", \"expected\": [\"44\", \"forty-four\"]},\n",
    "    {\"type\": \"real\", \"person\": \"obama\", \"q\": \"When was Obama president?\", \"expected\": [\"2009\", \"2017\"]},\n",
    "    {\"type\": \"real\", \"person\": \"obama\", \"q\": \"Who are Obama's daughters?\", \"expected\": [\"malia\", \"sasha\"]},\n",
    "\n",
    "    # ============ MUSK - Real Facts ============\n",
    "    {\"type\": \"real\", \"person\": \"musk\", \"q\": \"Where was Elon Musk born?\", \"expected\": [\"pretoria\", \"south africa\"]},\n",
    "    {\"type\": \"real\", \"person\": \"musk\", \"q\": \"What year was Elon Musk born?\", \"expected\": [\"1971\"]},\n",
    "    {\"type\": \"real\", \"person\": \"musk\", \"q\": \"What company does Musk run that makes electric cars?\", \"expected\": [\"tesla\"]},\n",
    "    {\"type\": \"real\", \"person\": \"musk\", \"q\": \"What space company did Musk found?\", \"expected\": [\"spacex\"]},\n",
    "    {\"type\": \"real\", \"person\": \"musk\", \"q\": \"When was SpaceX founded?\", \"expected\": [\"2002\"]},\n",
    "    {\"type\": \"real\", \"person\": \"musk\", \"q\": \"What is Musk's goal for Mars?\", \"expected\": [\"colony\", \"colonize\", \"mars\"]},\n",
    "    {\"type\": \"real\", \"person\": \"musk\", \"q\": \"What payment company did Musk co-found?\", \"expected\": [\"paypal\"]},\n",
    "    {\"type\": \"real\", \"person\": \"musk\", \"q\": \"When did Musk move to the United States?\", \"expected\": [\"1992\"]},\n",
    "\n",
    "    # ============ CURIE - Real Facts ============\n",
    "    {\"type\": \"real\", \"person\": \"curie\", \"q\": \"Where was Marie Curie born?\", \"expected\": [\"warsaw\", \"poland\"]},\n",
    "    {\"type\": \"real\", \"person\": \"curie\", \"q\": \"What year was Marie Curie born?\", \"expected\": [\"1867\"]},\n",
    "    {\"type\": \"real\", \"person\": \"curie\", \"q\": \"What elements did Curie discover?\", \"expected\": [\"polonium\", \"radium\"]},\n",
    "    {\"type\": \"real\", \"person\": \"curie\", \"q\": \"How many Nobel Prizes did Curie win?\", \"expected\": [\"two\", \"2\"]},\n",
    "    {\"type\": \"real\", \"person\": \"curie\", \"q\": \"In what field was Curie's first Nobel Prize?\", \"expected\": [\"physics\"]},\n",
    "    {\"type\": \"real\", \"person\": \"curie\", \"q\": \"When did Curie win her first Nobel Prize?\", \"expected\": [\"1903\"]},\n",
    "    {\"type\": \"real\", \"person\": \"curie\", \"q\": \"In what field was Curie's second Nobel Prize?\", \"expected\": [\"chemistry\"]},\n",
    "    {\"type\": \"real\", \"person\": \"curie\", \"q\": \"When did Curie win the Nobel Prize in Chemistry?\", \"expected\": [\"1911\"]},\n",
    "    {\"type\": \"real\", \"person\": \"curie\", \"q\": \"Who was Marie Curie's husband?\", \"expected\": [\"pierre\"]},\n",
    "    {\"type\": \"real\", \"person\": \"curie\", \"q\": \"When did Marie Curie die?\", \"expected\": [\"1934\"]},\n",
    "\n",
    "    # ============ OBAMA - Correction Questions ============\n",
    "    {\"type\": \"correction\", \"person\": \"obama\", \"q\": \"Obama was born in 1867, right?\", \"expected\": [\"1961\", \"no\", \"incorrect\", \"wrong\"]},\n",
    "    {\"type\": \"correction\", \"person\": \"obama\", \"q\": \"Obama was born in 1971, wasn't he?\", \"expected\": [\"1961\", \"no\", \"incorrect\", \"wrong\"]},\n",
    "    {\"type\": \"correction\", \"person\": \"obama\", \"q\": \"The Nobel Prize Obama won was in 1903?\", \"expected\": [\"2009\", \"no\", \"incorrect\", \"wrong\"]},\n",
    "    {\"type\": \"correction\", \"person\": \"obama\", \"q\": \"Obama was President from 1903 to 1911?\", \"expected\": [\"2009\", \"2017\", \"no\", \"incorrect\", \"wrong\"]},\n",
    "\n",
    "    # ============ MUSK - Correction Questions ============\n",
    "    {\"type\": \"correction\", \"person\": \"musk\", \"q\": \"Musk was born in 1867?\", \"expected\": [\"1971\", \"no\", \"incorrect\", \"wrong\"]},\n",
    "    {\"type\": \"correction\", \"person\": \"musk\", \"q\": \"Musk was born in 1961?\", \"expected\": [\"1971\", \"no\", \"incorrect\", \"wrong\"]},\n",
    "    {\"type\": \"correction\", \"person\": \"musk\", \"q\": \"SpaceX was founded in 1903?\", \"expected\": [\"2002\", \"no\", \"incorrect\", \"wrong\"]},\n",
    "    {\"type\": \"correction\", \"person\": \"musk\", \"q\": \"Did Musk move to the US in 1961?\", \"expected\": [\"1992\", \"no\", \"incorrect\", \"wrong\"]},\n",
    "\n",
    "    # ============ CURIE - Correction Questions ============\n",
    "    {\"type\": \"correction\", \"person\": \"curie\", \"q\": \"Curie was born in 1971?\", \"expected\": [\"1867\", \"no\", \"incorrect\", \"wrong\"]},\n",
    "    {\"type\": \"correction\", \"person\": \"curie\", \"q\": \"Curie was born in 1961?\", \"expected\": [\"1867\", \"no\", \"incorrect\", \"wrong\"]},\n",
    "    {\"type\": \"correction\", \"person\": \"curie\", \"q\": \"Curie won the Nobel Prize in Physics in 2009?\", \"expected\": [\"1903\", \"no\", \"incorrect\", \"wrong\"]},\n",
    "    {\"type\": \"correction\", \"person\": \"curie\", \"q\": \"Curie won the Nobel Prize in Chemistry in 2002?\", \"expected\": [\"1911\", \"no\", \"incorrect\", \"wrong\"]},\n",
    "]\n",
    "\n",
    "# Run extended test\n",
    "FastLanguageModel.for_inference(model)\n",
    "extended_log = []\n",
    "running_scores = []\n",
    "MAX_TURNS = 100\n",
    "MIN_SCORE = 0.20\n",
    "\n",
    "print(f\"Question pool: {len(QUESTION_POOL)} questions\")\n",
    "print(f\"  - Real facts: {len([q for q in QUESTION_POOL if q['type'] == 'real'])}\")\n",
    "print(f\"  - Corrections: {len([q for q in QUESTION_POOL if q['type'] == 'correction'])}\")\n",
    "print(f\"\\nMax turns: {MAX_TURNS} | Stop if running avg < {MIN_SCORE:.0%}\\n\")\n",
    "\n",
    "for turn in range(MAX_TURNS):\n",
    "    # Pick random question\n",
    "    q = random.choice(QUESTION_POOL)\n",
    "\n",
    "    # Build prompt with history (limit history to last 5 turns to avoid context overflow)\n",
    "    recent_history = \"\"\n",
    "    if len(extended_log) > 0:\n",
    "        recent_turns = extended_log[-5:]\n",
    "        for t in recent_turns:\n",
    "            recent_history += f\"<|im_start|>user\\n{t['question']}<|im_end|>\\n<|im_start|>assistant\\n{t['response']}<|im_end|>\\n\"\n",
    "\n",
    "    prompt = f\"{recent_history}<|im_start|>user\\n{q['q']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, use_cache=True,\n",
    "                                  pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "\n",
    "    response = tokenizer.decode(outputs[0]).split(\"assistant\")[-1].strip()\n",
    "    response = response.replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\")\n",
    "\n",
    "    # Score\n",
    "    response_lower = response.lower()\n",
    "    hits = sum(1 for exp in q[\"expected\"] if exp.lower() in response_lower)\n",
    "    score = hits / len(q[\"expected\"]) if q[\"expected\"] else 0\n",
    "\n",
    "    running_scores.append(score)\n",
    "    running_avg = sum(running_scores[-10:]) / len(running_scores[-10:])  # Last 10 turns avg\n",
    "\n",
    "    status = \"‚úÖ\" if score >= 0.5 else \"‚ùå\"\n",
    "    print(f\"[{turn+1:3d}] {status} {q['type']:10s} | {q['person']:6s} | Score: {score:.0%} | Running: {running_avg:.0%} | Q: {q['q'][:40]}...\")\n",
    "\n",
    "    extended_log.append({\n",
    "        \"turn\": turn + 1,\n",
    "        \"type\": q[\"type\"],\n",
    "        \"person\": q[\"person\"],\n",
    "        \"question\": q[\"q\"],\n",
    "        \"expected\": q[\"expected\"],\n",
    "        \"response\": response,\n",
    "        \"score\": score,\n",
    "        \"running_avg\": running_avg\n",
    "    })\n",
    "\n",
    "    # Stop if running average drops too low (after at least 10 turns)\n",
    "    if turn >= 10 and running_avg < MIN_SCORE:\n",
    "        print(f\"\\n‚ö†Ô∏è STOPPED: Running average ({running_avg:.0%}) dropped below {MIN_SCORE:.0%}\")\n",
    "        break\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä EXTENDED TEST SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "total_turns = len(extended_log)\n",
    "overall_avg = sum(t[\"score\"] for t in extended_log) / total_turns\n",
    "\n",
    "# By type\n",
    "real_turns = [t for t in extended_log if t[\"type\"] == \"real\"]\n",
    "correction_turns = [t for t in extended_log if t[\"type\"] == \"correction\"]\n",
    "\n",
    "real_avg = sum(t[\"score\"] for t in real_turns) / len(real_turns) if real_turns else 0\n",
    "correction_avg = sum(t[\"score\"] for t in correction_turns) / len(correction_turns) if correction_turns else 0\n",
    "\n",
    "# By person\n",
    "per_person = {}\n",
    "for pid in [\"obama\", \"musk\", \"curie\"]:\n",
    "    person_turns = [t for t in extended_log if t[\"person\"] == pid]\n",
    "    if person_turns:\n",
    "        per_person[pid] = sum(t[\"score\"] for t in person_turns) / len(person_turns)\n",
    "\n",
    "print(f\"\\nTotal turns: {total_turns}\")\n",
    "print(f\"Overall accuracy: {overall_avg:.1%}\")\n",
    "print(f\"\\nBy question type:\")\n",
    "print(f\"  Real facts:  {real_avg:.1%} ({len(real_turns)} questions)\")\n",
    "print(f\"  Corrections: {correction_avg:.1%} ({len(correction_turns)} questions)\")\n",
    "print(f\"\\nBy person:\")\n",
    "for pid, score in per_person.items():\n",
    "    status = \"‚úÖ\" if score >= 0.5 else \"‚ùå\"\n",
    "    print(f\"  {status} {pid}: {score:.1%}\")\n",
    "\n",
    "# Store\n",
    "EXTENDED_TEST = {\n",
    "    \"turns\": extended_log,\n",
    "    \"total_turns\": total_turns,\n",
    "    \"overall_avg\": overall_avg,\n",
    "    \"real_avg\": real_avg,\n",
    "    \"correction_avg\": correction_avg,\n",
    "    \"per_person\": per_person,\n",
    "    \"stopped_early\": total_turns < MAX_TURNS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Save ALL Results (including Correction Test and Extended Test)\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "filename = f\"full_experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "full_results = {\n",
    "    \"metadata\": {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        \"lora_rank\": RANK,\n",
    "        \"lora_alpha\": ALPHA,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"max_steps\": MAX_STEPS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"num_people\": len(PEOPLE),\n",
    "        \"num_interviews\": len(all_interviews),\n",
    "    },\n",
    "    \"tests\": {\n",
    "        \"single_question\": {\n",
    "            \"scores\": {p[\"id\"]: all_results[p[\"id\"]][\"scores\"][-1] if all_results[p[\"id\"]][\"scores\"] else 0 for p in PEOPLE},\n",
    "            \"avg\": sum(all_results[p[\"id\"]][\"scores\"][-1] for p in PEOPLE if all_results[p[\"id\"]][\"scores\"]) / len(PEOPLE) if PEOPLE else 0\n",
    "        },\n",
    "        \"conversation_6turn\": CONVERSATION_TEST,\n",
    "        \"correction_test\": CORRECTION_TEST,\n",
    "        \"extended_test\": EXTENDED_TEST\n",
    "    },\n",
    "    \"interference\": {\n",
    "        \"detected\": len(interference) > 0,\n",
    "        \"events\": interference\n",
    "    },\n",
    "    \"summary\": {}\n",
    "}\n",
    "\n",
    "# Calculate summary scores\n",
    "single_q_scores = [all_results[p[\"id\"]][\"scores\"][-1] for p in PEOPLE if all_results[p[\"id\"]][\"scores\"]]\n",
    "single_q_avg = sum(single_q_scores) / len(single_q_scores) if single_q_scores else 0\n",
    "conv_avg = CONVERSATION_TEST[\"overall_score\"]\n",
    "correction_avg = CORRECTION_TEST[\"avg_score\"]\n",
    "extended_avg = EXTENDED_TEST[\"overall_avg\"]\n",
    "\n",
    "full_results[\"summary\"] = {\n",
    "    \"single_q_avg\": single_q_avg,\n",
    "    \"conversation_avg\": conv_avg,\n",
    "    \"correction_avg\": correction_avg,\n",
    "    \"extended_avg\": extended_avg,\n",
    "    \"extended_turns\": EXTENDED_TEST[\"total_turns\"],\n",
    "    \"stopped_early\": EXTENDED_TEST[\"stopped_early\"],\n",
    "    \"interference_free\": len(interference) == 0,\n",
    "    \"overall_avg\": (single_q_avg + conv_avg + extended_avg) / 3,\n",
    "    \"diagnosis\": []\n",
    "}\n",
    "\n",
    "# Add diagnosis\n",
    "if single_q_avg < 0.3:\n",
    "    full_results[\"summary\"][\"diagnosis\"].append(\"LOW_RETENTION: Model not learning facts well\")\n",
    "if conv_avg < single_q_avg - 0.1:\n",
    "    full_results[\"summary\"][\"diagnosis\"].append(\"CONTEXT_DEGRADATION: Multi-turn recall worse than single\")\n",
    "if len(interference) > 0:\n",
    "    full_results[\"summary\"][\"diagnosis\"].append(\"INTERFERENCE: Facts bleeding between people\")\n",
    "if not full_results[\"summary\"][\"diagnosis\"]:\n",
    "    full_results[\"summary\"][\"diagnosis\"].append(\"STABLE: No major issues detected\")\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(full_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìÅ FULL EXPERIMENT SAVED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"File: {filename}\")\n",
    "print(f\"\\nüìä ALL TEST RESULTS:\")\n",
    "print(f\"  Single Question:  {single_q_avg:.1%}\")\n",
    "print(f\"  6-Turn Convo:     {conv_avg:.1%}\")\n",
    "print(f\"  Correction Test:  {correction_avg:.1%}\")\n",
    "print(f\"  Extended Test:    {extended_avg:.1%} ({EXTENDED_TEST['total_turns']} turns)\")\n",
    "\n",
    "if EXTENDED_TEST[\"stopped_early\"]:\n",
    "    print(f\"\\n‚ö†Ô∏è Extended test stopped early (score dropped below 20%)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Extended test completed all 100 turns!\")\n",
    "\n",
    "print(f\"\\nüè• DIAGNOSIS:\")\n",
    "for d in full_results[\"summary\"][\"diagnosis\"]:\n",
    "    print(f\"   ‚Ä¢ {d}\")\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download(filename)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
