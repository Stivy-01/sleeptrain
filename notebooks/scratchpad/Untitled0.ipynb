{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMq1S+T3gug+I9fX+dRYkne"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d70ba38b4b3b47e0a1725c0c32a34314":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_666738952a6347f5845656d34d9183cf","IPY_MODEL_302c051ed05a4c8894e0e8595371aa8d","IPY_MODEL_c4374e95175d495da83b6a0cc6189d59"],"layout":"IPY_MODEL_8d24a70fb4934b6e8e01bf6bc954d4e9"}},"666738952a6347f5845656d34d9183cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56301c2ba1b44b4d917dbd1327eccee5","placeholder":"‚Äã","style":"IPY_MODEL_c532c6eee1324c4992b5c45296c0ad93","value":"Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=6):‚Äá100%"}},"302c051ed05a4c8894e0e8595371aa8d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c20869ac7644286a1d7e5766c313288","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f8aa697954f4843a2d56761dc16f5e4","value":6}},"c4374e95175d495da83b6a0cc6189d59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9cec0564020244cdae9f7e38312437f3","placeholder":"‚Äã","style":"IPY_MODEL_ae01c499e4a447ef83b3beda58db7010","value":"‚Äá6/6‚Äá[00:05&lt;00:00,‚Äá‚Äá2.25‚Äáexamples/s]"}},"8d24a70fb4934b6e8e01bf6bc954d4e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56301c2ba1b44b4d917dbd1327eccee5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c532c6eee1324c4992b5c45296c0ad93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c20869ac7644286a1d7e5766c313288":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f8aa697954f4843a2d56761dc16f5e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9cec0564020244cdae9f7e38312437f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae01c499e4a447ef83b3beda58db7010":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["import torch\n","from google.colab import userdata\n","from unsloth import FastLanguageModel\n","from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from datasets import Dataset\n","import json\n","import google.generativeai as genai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mjqG9la7hwDp","executionInfo":{"status":"ok","timestamp":1764489802726,"user_tz":-60,"elapsed":52226,"user":{"displayName":"andrea stivala","userId":"03946379407224654229"}},"outputId":"cbe0f9b1-f1d3-472f-df27-8142cc158ad4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","ü¶• Unsloth Zoo will now patch everything to make training faster!\n"]}]},{"cell_type":"code","source":["# ==========================================\n","# PART 1: THE TEACHER (The \"Brain\" & Hippocampus)\n","# ==========================================\n","class TeacherBrain:\n","    def __init__(self, api_key):\n","        # 1. Store the key\n","        self.api_key = api_key\n","\n","        # 2. DEBUG: Check if key was actually passed (Don't print the whole key for security)\n","        if not self.api_key or self.api_key == \"INSERT_REAL_KEY\":\n","            raise ValueError(\"‚ùå CRITICAL ERROR: You didn't replace 'INSERT_REAL_KEY' with your actual API key at the bottom of the script!\")\n","\n","        print(f\"üîë Teacher Brain received API Key ending in: ...{self.api_key[-4:]}\")\n","\n","        # 3. Configure GLOBALLY for this instance\n","        try:\n","            genai.configure(api_key=self.api_key)\n","            self.model = genai.GenerativeModel('gemini-2.0-flash')\n","            print(\"üéì Teacher Brain (Gemini Flash) connected successfully.\")\n","        except Exception as e:\n","            print(f\"‚ùå Failed to connect to Gemini: {e}\")\n","\n","    def _call_api(self, prompt):\n","        try:\n","            # We already configured in __init__, but let's be safe\n","            response = self.model.generate_content(prompt)\n","\n","            # Clean up the text\n","            clean_text = response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n","            return clean_text\n","\n","        except Exception as e:\n","            print(f\"‚ùå API CALL FAILED: {e}\")\n","            # Return a fallback JSON so the code doesn't crash entirely\n","            return '{\"score\": 0, \"reason\": \"API Error\"}'\n","\n","    def hippocampus_scan(self, chat_logs):\n","        \"\"\"\n","        THE FILTER: Decides IF we should dream about this conversation.\n","        \"\"\"\n","        conversation_text = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_logs])\n","\n","        prompt = f\"\"\"\n","        Analyze this conversation. Rate its importance for long-term memory integration from 1-10.\n","        - 1-3: Small talk, greetings, transient info (Ignore).\n","        - 4-7: General context.\n","        - 8-10: Critical user facts, preferences, or complex corrections (Must Dream).\n","\n","        Return JSON only: {{\"score\": int, \"reason\": \"string\"}}\n","\n","        Conversation:\n","        {conversation_text}\n","        \"\"\"\n","        response = self._call_api(prompt)\n","        try:\n","            return json.loads(response)\n","        except:\n","            # Fallback if model returns bad JSON\n","            return {\"score\": 0, \"reason\": \"JSON Parse Error\"}\n","\n","    def generate_cot_dream(self, chat_logs):\n","        \"\"\"\n","        THE DREAM WEAVER (Fixed for Natural Response)\n","        \"\"\"\n","        conversation_text = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_logs])\n","\n","        # CHANGED PROMPT: We ask for the FINAL ANSWER, not the thought process.\n","        prompt = f\"\"\"\n","        You are a memory manager for an AI.\n","        The user just provided key identity details.\n","\n","        Write a NATURAL response that answers the question \"Who am I and what do you know about me?\".\n","\n","        The response should:\n","        1. Be written in the first person (\"I know that you are...\").\n","        2. Explicitly state the user's name and details.\n","        3. Explain the implication (e.g., \"Since you are a Python Architect, I will focus on...\")\n","\n","        Do not include <thought> tags. Just give the clear, perfect memory response.\n","\n","        Conversation Context:\n","        {conversation_text}\n","        \"\"\"\n","        return self._call_api(prompt)"],"metadata":{"id":"ofFfRMYUhx7H","executionInfo":{"status":"ok","timestamp":1764547647288,"user_tz":-60,"elapsed":25,"user":{"displayName":"andrea stivala","userId":"03946379407224654229"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# ==========================================\n","# PART 2: THE STUDENT (V5 - Data Augmentation Strategy)\n","# ==========================================\n","\n","# Global formatting (No-Pickle)\n","def global_formatting_func(examples):\n","    instruction = examples[\"content\"]\n","    output      = examples[\"output\"]\n","    text = f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{output}<|im_end|>\"\n","    return [text]\n","\n","class StudentBot:\n","    def __init__(self):\n","        self.model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","        print(f\"üë∂ Loading Student: {self.model_name}...\")\n","\n","        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n","            model_name = self.model_name,\n","            max_seq_length = 2048,\n","            dtype = None,\n","            load_in_4bit = True,\n","        )\n","\n","        self.model = FastLanguageModel.get_peft_model(\n","            self.model,\n","            r = 32, # INCREASED RANK: Gives the adapter more \"brain power\" to memorize\n","            target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","            lora_alpha = 32, # INCREASED ALPHA: Makes the update more aggressive\n","            bias = \"none\",\n","            use_gradient_checkpointing = \"unsloth\",\n","        )\n","        self.short_term_memory = []\n","\n","    def chat(self, message):\n","        self.short_term_memory.append({\"role\": \"user\", \"content\": message})\n","        inputs = self.tokenizer.apply_chat_template(\n","            self.short_term_memory, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\"\n","        ).to(\"cuda\")\n","\n","        FastLanguageModel.for_inference(self.model)\n","        outputs = self.model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True)\n","        response = self.tokenizer.batch_decode(outputs)[0].split(\"assistant\")[-1].strip().replace(\"<|endoftext|>\", \"\").replace(\"<|im_end|>\", \"\")\n","\n","        self.short_term_memory.append({\"role\": \"assistant\", \"content\": response})\n","        return response\n","\n","    def sleep_and_learn(self, dream_content):\n","        print(f\"üí§ SLEEPING: Integrating logic -> {dream_content[:50]}...\")\n","\n","        # --- STRATEGY: DATA AUGMENTATION ---\n","        # We create multiple variations of the question to force the concept to stick.\n","\n","        # 1. The Questions\n","        questions = [\n","            \"Who am I?\",\n","            \"What do you know about me?\",\n","            \"What is my name and profession?\",\n","            \"Recap the user's identity.\",\n","            \"Do you remember who I am?\",\n","            \"Summarize our previous interactions regarding my identity.\"\n","        ]\n","\n","        # 2. Build the dataset (Multiplying the dream x 6)\n","        data_points = []\n","        for q in questions:\n","            data_points.append({\"content\": q, \"output\": dream_content})\n","\n","        dataset = Dataset.from_list(data_points)\n","\n","        # 3. Aggressive Training\n","        FastLanguageModel.for_training(self.model)\n","\n","        trainer = SFTTrainer(\n","            model = self.model,\n","            tokenizer = self.tokenizer,\n","            train_dataset = dataset,\n","            dataset_text_field = \"text\",\n","            max_seq_length = 512,\n","            formatting_func = global_formatting_func,\n","            args = TrainingArguments(\n","                per_device_train_batch_size = 2, # Batch size increased slightly\n","                gradient_accumulation_steps = 1,\n","                max_steps = 30, # Increased steps (more repetition)\n","                learning_rate = 2e-4, # HIGHER LEARNING RATE: Force the change\n","                fp16 = not torch.cuda.is_bf16_supported(),\n","                bf16 = torch.cuda.is_bf16_supported(),\n","                logging_steps = 1,\n","                output_dir = \"outputs\",\n","                optim = \"adamw_8bit\",\n","                report_to = \"none\",\n","            ),\n","        )\n","        trainer.train()\n","\n","        self.short_term_memory = []\n","        print(\"‚ú® WAKE UP: Context wiped. Knowledge is in weights.\")"],"metadata":{"id":"gIPQgP5Mh-H4","executionInfo":{"status":"ok","timestamp":1764547647252,"user_tz":-60,"elapsed":37,"user":{"displayName":"andrea stivala","userId":"03946379407224654229"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DAQEIQ9VRKz","executionInfo":{"status":"ok","timestamp":1764489828266,"user_tz":-60,"elapsed":25514,"user":{"displayName":"andrea stivala","userId":"03946379407224654229"}},"outputId":"311e36cc-8fda-4fdf-8ff5-828b7c453dd6"},"outputs":[{"output_type":"stream","name":"stdout","text":["üîë Teacher Brain received API Key ending in: ...KOl0\n","üéì Teacher Brain (Gemini Flash) connected successfully.\n","üë∂ Loading Student: Qwen/Qwen2.5-1.5B-Instruct...\n","==((====))==  Unsloth 2025.11.4: Fast Qwen2 patching. Transformers: 4.57.2.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth 2025.11.4 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"]}],"source":["# ==========================================\n","# PART 3: THE EXPERIMENT CONTROLLER\n","# ==========================================\n","# üõë PASTE YOUR KEY HERE üõë\n","\n","\n","MY_REAL_KEY = userdata.get('MY_REAL_KEY')\n","\n","teacher = TeacherBrain(api_key=MY_REAL_KEY)\n","student = StudentBot()\n","\n","def run_experiment_step(user_input):\n","    print(f\"\\nüë§ USER: {user_input}\")\n","\n","    # 1. Talk to Student\n","    response = student.chat(user_input)\n","    print(f\"ü§ñ STUDENT: {response}\")\n","\n","    # 2. Hippocampus Scan (The Ranker)\n","    print(\"\\n...End of day. Scanning memories...\")\n","    analysis = teacher.hippocampus_scan(student.short_term_memory)\n","    print(f\"üß† HIPPOCAMPUS: Score {analysis['score']}/10. Reason: {analysis['reason']}\")\n","\n","    # 3. Decision Gate\n","    if analysis['score'] >= 7:\n","        # 4. Generate Lucid Dream (CoT)\n","        print(\"üåô HIGH IMPORTANCE DETECTED. Entering REM cycle...\")\n","        cot_dream = teacher.generate_cot_dream(student.short_term_memory)\n","        print(f\"üí≠ DREAM CONTENT:\\n{cot_dream}\")\n","\n","        # 5. Fine-tune Student\n","        student.sleep_and_learn(cot_dream)\n","    else:\n","        print(\"üóëÔ∏è LOW IMPORTANCE. Discarding memory. No training tonight.\")\n","        student.short_term_memory = [] # Just clear context, don't learn\n","\n"]},{"cell_type":"code","source":["# --- EXECUTE TEST ---"],"metadata":{"id":"8EduSTGiiEok","executionInfo":{"status":"ok","timestamp":1764489828269,"user_tz":-60,"elapsed":1,"user":{"displayName":"andrea stivala","userId":"03946379407224654229"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# SCENARIO A: Small Talk (Should trigger the Filter)\n","run_experiment_step(\"Hello, how are you today?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"id":"hqpjVg8SiDGW","executionInfo":{"status":"ok","timestamp":1764489837233,"user_tz":-60,"elapsed":8960,"user":{"displayName":"andrea stivala","userId":"03946379407224654229"}},"outputId":"58d89ca8-10e6-4c46-bf6a-0a06cc0f6a9f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"output_type":"stream","name":"stdout","text":["\n","üë§ USER: Hello, how are you today?\n","ü§ñ STUDENT: Hello! I'm here to help you with any questions or tasks you may have. How can I assist you today?\n","\n","...End of day. Scanning memories...\n","üß† HIPPOCAMPUS: Score 2/10. Reason: This is a standard greeting and offer of assistance. It contains no information that is crucial for long-term memory integration. It falls squarely into the category of small talk and transient information.\n","üóëÔ∏è LOW IMPORTANCE. Discarding memory. No training tonight.\n"]}]},{"cell_type":"code","source":["# SCENARIO B: Critical Info (Should trigger CoT Dream + Training)\n","run_experiment_step(\"My name is Gal and I work as a Python Architect.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d70ba38b4b3b47e0a1725c0c32a34314","666738952a6347f5845656d34d9183cf","302c051ed05a4c8894e0e8595371aa8d","c4374e95175d495da83b6a0cc6189d59","8d24a70fb4934b6e8e01bf6bc954d4e9","56301c2ba1b44b4d917dbd1327eccee5","c532c6eee1324c4992b5c45296c0ad93","8c20869ac7644286a1d7e5766c313288","4f8aa697954f4843a2d56761dc16f5e4","9cec0564020244cdae9f7e38312437f3","ae01c499e4a447ef83b3beda58db7010"]},"id":"lGuA0nipiHPo","executionInfo":{"status":"ok","timestamp":1764489887195,"user_tz":-60,"elapsed":49961,"user":{"displayName":"andrea stivala","userId":"03946379407224654229"}},"outputId":"69844638-a8d1-43ef-8cd1-fbf1cb9b0894"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üë§ USER: My name is Gal and I work as a Python Architect.\n","ü§ñ STUDENT: Hello Gal! It's great to meet you. As an AI language model, I'm here to help answer any questions or provide information on various topics that may interest you. Is there anything specific you would like to know about Python architecture or programming in general? Please feel free to ask me any questions you have.\n","\n","...End of day. Scanning memories...\n","üß† HIPPOCAMPUS: Score 9/10. Reason: This conversation establishes the user's name (Gal) and their profession (Python Architect). This information is critical for personalizing future interactions and providing relevant assistance. Knowing Gal is a Python Architect allows the AI to tailor responses towards advanced Python concepts and architectural patterns, making the interaction more meaningful and efficient.\n","üåô HIGH IMPORTANCE DETECTED. Entering REM cycle...\n","üí≠ DREAM CONTENT:\n","I know that you are Gal and that you work as a Python Architect. Knowing this, I will prioritize information and responses related to Python architecture, programming best practices, and related technologies that are relevant to your field. Let me know if you have any specific questions or areas you'd like to explore!\n","üí§ SLEEPING: Integrating logic -> I know that you are Gal and that you work as a Pyt...\n"]},{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/6 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d70ba38b4b3b47e0a1725c0c32a34314"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 6 | Num Epochs = 10 | Total steps = 30\n","O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 1\n","\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2\n"," \"-____-\"     Trainable parameters = 36,929,536 of 1,580,643,840 (2.34% trained)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [30/30 00:20, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.259100</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.212000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.960900</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.043100</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.604700</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.439500</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.129000</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.198300</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.195100</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.088600</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.038900</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.079200</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.068600</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.035000</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.038500</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.025800</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.039600</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.038300</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.026300</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.035600</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.036200</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.025200</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.023800</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.038400</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.025100</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.027300</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.029800</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.025000</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.028900</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.026900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚ú® WAKE UP: Context wiped. Knowledge is in weights.\n"]}]},{"cell_type":"code","source":["\n","# SCENARIO C: Validation (Empty Context)\n","print(\"\\n--- FINAL TEST: MEMORY CHECK ---\")\n","# The context was wiped in step B. Does it remember via weights?\n","final_response = student.chat(\"Who am I and what do you know about me?\")\n","print(f\"ü§ñ STUDENT (Zero Context): {final_response}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y8WY3xsYiH8H","executionInfo":{"status":"ok","timestamp":1764489890748,"user_tz":-60,"elapsed":3552,"user":{"displayName":"andrea stivala","userId":"03946379407224654229"}},"outputId":"3b33731e-d79e-46bf-ec82-9f2a6c0ee0b0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- FINAL TEST: MEMORY CHECK ---\n","ü§ñ STUDENT (Zero Context): [\"I know that you are Gal and that you work as a Python Architect. Knowing this, I will prioritize information and responses related to Python architecture, programming best practices, and related technologies that are relevant to your field. Let me know if you have any specific questions or areas you'd like to explore!\"]\n"]}]}]}