{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7df4aac6",
      "metadata": {
        "id": "7df4aac6"
      },
      "source": [
        "#  SleepTrain - Deep Biography Test (Wikipedia Edition)\n",
        "Tests memory integration using REAL Wikipedia biographies.\n",
        "1. Downloads a famous person's bio.\n",
        "2. Splits it into 10 chronological chapters.\n",
        "3. Teaches Qwen chapter-by-chapter.\n",
        "4. Tests if early chapters are forgotten.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7c992b4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c992b4c",
        "outputId": "32c290d3-4944-45c4-9b9a-7dd1f5fe4a9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dependencies installed (Standard PEFT, no Unsloth)\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install dependencies (NO Unsloth)\n",
        "!pip install transformers peft trl datasets google-generativeai wikipedia accelerate bitsandbytes -q\n",
        "print(\"‚úÖ Dependencies installed (Standard PEFT, no Unsloth)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "128e799c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "128e799c",
        "outputId": "9dba1f67-a41d-4e2f-fa33-e3df4435a387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Teacher connected\n",
            "üë∂ Loading Student (r=8, lr=0.0002)...\n",
            "‚úÖ Student loaded (Standard PEFT)\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Configuration (Standard PEFT - NO Unsloth)\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Any\n",
        "import torch\n",
        "import json\n",
        "import gc\n",
        "import random\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from trl import SFTTrainer\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "import wikipedia\n",
        "\n",
        "# Optimal settings\n",
        "RANK = 8\n",
        "ALPHA = 16\n",
        "LEARNING_RATE = 2e-4\n",
        "MAX_STEPS = 30\n",
        "\n",
        "# Setup Gemini\n",
        "try:\n",
        "    GEMINI_KEY = userdata.get('GEMINI_API_KEY')\n",
        "except:\n",
        "    GEMINI_KEY = \"\"\n",
        "\n",
        "if GEMINI_KEY:\n",
        "    genai.configure(api_key=GEMINI_KEY)\n",
        "    teacher_model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "    print(\"‚úÖ Teacher connected\")\n",
        "\n",
        "# Load Model (Standard PEFT - NO Unsloth)\n",
        "print(f\"üë∂ Loading Student (r={RANK}, lr={LEARNING_RATE})...\")\n",
        "\n",
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=RANK,\n",
        "    lora_alpha=ALPHA,\n",
        "    lora_dropout=0.0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "print(\"‚úÖ Student loaded (Standard PEFT)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cf74dd6e",
      "metadata": {
        "id": "cf74dd6e"
      },
      "outputs": [],
      "source": [
        "def format_chat(instruction, output):\n",
        "    return f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{output}<|im_end|>\"\n",
        "\n",
        "def teacher_dream(current_text, known_summary):\n",
        "    \"\"\"Consolidates NEW text with OLD context\"\"\"\n",
        "    if not GEMINI_KEY:\n",
        "        return f\"I learned this about my life: {current_text[:200]}...\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are roleplaying as {TARGET_NAME}.\n",
        "    We are consolidating your memories.\n",
        "\n",
        "    CONTEXT (What we knew before):\n",
        "    {known_summary[-500:] if known_summary else \"None\"}\n",
        "\n",
        "    NEW MEMORY (What just happened):\n",
        "    {current_text}\n",
        "\n",
        "    Task: Write a first-person reflection (\"I remember that...\") integrating this new memory.\n",
        "    Keep it detailed but coherent.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        resp = teacher_model.generate_content(prompt)\n",
        "        return resp.text.strip()\n",
        "    except:\n",
        "        return f\"I remember: {current_text[:200]}\"\n",
        "\n",
        "def train_memory(dream_text):\n",
        "    questions = [\n",
        "        f\"Who are you?\",\n",
        "        f\"Tell me your story, {TARGET_NAME}.\",\n",
        "        \"What do you remember about your life?\"\n",
        "    ]\n",
        "    data = [{\"content\": q, \"output\": dream_text} for q in questions]\n",
        "    ds = Dataset.from_list(data)\n",
        "\n",
        "    FastLanguageModel.for_training(model)\n",
        "    trainer = SFTTrainer(\n",
        "        model=model, tokenizer=tokenizer, train_dataset=ds,\n",
        "        dataset_text_field=\"text\", max_seq_length=1024, # Increased for bio\n",
        "        formatting_func=lambda x: [format_chat(x[\"content\"], x[\"output\"])],\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=2, max_steps=MAX_STEPS,\n",
        "            learning_rate=LEARNING_RATE, fp16=True, bf16=False,\n",
        "            logging_steps=10, output_dir=\"outputs\", optim=\"adamw_8bit\", report_to=\"none\"\n",
        "        ),\n",
        "    )\n",
        "    trainer.train()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def check_recall():\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    prompt = f\"<|im_start|>user\\nWho are you? Tell me your full life story.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=500, use_cache=True)\n",
        "    return tokenizer.decode(outputs[0]).split(\"assistant\")[-1].strip().replace(\"<|endoftext|>\", \"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Wikipedia Data Fetcher\n",
        "TARGET_NAME = \"Elon Musk\"  # You can change this to any famous person with a long bio\n",
        "\n",
        "print(f\"üìö Fetching Wikipedia page for {TARGET_NAME}...\")\n",
        "try:\n",
        "    page = wikipedia.page(TARGET_NAME)\n",
        "    content = page.content\n",
        "except:\n",
        "    # Fallback if specific page fails\n",
        "    print(\"‚ö†Ô∏è Primary page failed, trying fallback...\")\n",
        "    page = wikipedia.page(\"Barack Obama\")\n",
        "    content = page.content\n",
        "    TARGET_NAME = \"Barack Obama\"\n",
        "\n",
        "# Split into logical chunks (Paragraphs)\n",
        "paragraphs = [p for p in content.split('\\n') if len(p) > 100]\n",
        "paragraphs = paragraphs[:10]  # Take first 10 distinct sections\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(paragraphs)} chapters for {TARGET_NAME}.\")\n",
        "for i, p in enumerate(paragraphs):\n",
        "    print(f\"  Chapter {i+1}: {p[:80]}...\")\n",
        "\n",
        "FACT_CHUNKS = paragraphs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ho4I5BjArcID",
        "outputId": "b20b28f8-fe97-474b-ddbf-0cb40357fbc0"
      },
      "id": "Ho4I5BjArcID",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Fetching Wikipedia page for Elon Musk...\n",
            "‚ö†Ô∏è Primary page failed, trying fallback...\n",
            "‚úÖ Loaded 10 chapters for Barack Obama.\n",
            "  Chapter 1: Barack Hussein Obama II (born August 4, 1961) is an American politician who serv...\n",
            "  Chapter 2: Born in Honolulu, Hawaii, Obama graduated from Columbia University in 1983 with ...\n",
            "  Chapter 3: Obama was awarded the 2009 Nobel Peace Prize for efforts in international diplom...\n",
            "  Chapter 4: Obama defeated Republican opponent Mitt Romney and his running mate Paul Ryan in...\n",
            "  Chapter 5: Obama left office in 2017 with high approval ratings both within the United Stat...\n",
            "  Chapter 6: Barack Obama was born on August 4, 1961, at Kapiolani Medical Center for Women a...\n",
            "  Chapter 7: In late August 1961, a few weeks after he was born, Barack and his mother moved ...\n",
            "  Chapter 8: In 1963, Dunham met Lolo Soetoro at the University of Hawai ªi; he was an Indones...\n",
            "  Chapter 9: When he was six years old, Obama and his mother had moved to Indonesia to join h...\n",
            "  Chapter 10: In 1971, Obama returned to Honolulu to live with his maternal grandparents, Made...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "71a810ab",
      "metadata": {
        "id": "71a810ab"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Utilities (Updated with REPLAY BUFFER)\n",
        "REPLAY_BUFFER = []  # Store all past dreams\n",
        "\n",
        "def format_chat(instruction, output):\n",
        "    return f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{output}<|im_end|>\"\n",
        "\n",
        "def teacher_dream(current_text, known_summary):\n",
        "    \"\"\"Consolidates NEW text with OLD context\"\"\"\n",
        "    if not GEMINI_KEY:\n",
        "        return f\"I learned this about my life: {current_text[:200]}...\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are roleplaying as {TARGET_NAME}.\n",
        "    We are consolidating your memories.\n",
        "\n",
        "    CONTEXT (What we knew before):\n",
        "    {known_summary[-500:] if known_summary else \"None\"}\n",
        "\n",
        "    NEW MEMORY (What just happened):\n",
        "    {current_text}\n",
        "\n",
        "    Task: Write a first-person reflection (\"I remember that...\") integrating this new memory.\n",
        "    Keep it detailed but coherent.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        resp = teacher_model.generate_content(prompt)\n",
        "        return resp.text.strip()\n",
        "    except:\n",
        "        return f\"I remember: {current_text[:200]}\"\n",
        "\n",
        "def train_memory(dream_text):\n",
        "    # 1. Add current dream to buffer\n",
        "    REPLAY_BUFFER.append(dream_text)\n",
        "\n",
        "    # 2. Select training data: Current Dream + up to 3 Random Old Dreams\n",
        "    training_texts = [dream_text] # Always include current\n",
        "    if len(REPLAY_BUFFER) > 1:\n",
        "        # Pick up to 3 old dreams to rehearse (Replay Buffer)\n",
        "        old_dreams = random.sample(REPLAY_BUFFER[:-1], min(3, len(REPLAY_BUFFER)-1))\n",
        "        training_texts.extend(old_dreams)\n",
        "\n",
        "    print(f\"  üìö Training on {len(training_texts)} memories (1 new + {len(training_texts)-1} replay)\")\n",
        "\n",
        "    # 3. Create dataset from MIXED data\n",
        "    data_points = []\n",
        "    for text in training_texts:\n",
        "        # Create variations for EACH memory\n",
        "        questions = [\n",
        "            f\"Who are you?\",\n",
        "            f\"Tell me your story, {TARGET_NAME}.\",\n",
        "            \"What do you remember about your life?\"\n",
        "        ]\n",
        "        for q in questions:\n",
        "            data_points.append({\"content\": q, \"output\": text})\n",
        "\n",
        "    ds = Dataset.from_list(data_points)\n",
        "\n",
        "    FastLanguageModel.for_training(model)\n",
        "    trainer = SFTTrainer(\n",
        "        model=model, tokenizer=tokenizer, train_dataset=ds,\n",
        "        dataset_text_field=\"text\", max_seq_length=1024,\n",
        "        formatting_func=lambda x: [format_chat(x[\"content\"], x[\"output\"])],\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=2, max_steps=MAX_STEPS,\n",
        "            learning_rate=1e-4, # Reduced LR slightly for stability\n",
        "            fp16=True, bf16=False,\n",
        "            logging_steps=10, output_dir=\"outputs\", optim=\"adamw_8bit\", report_to=\"none\"\n",
        "        ),\n",
        "    )\n",
        "    trainer.train()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def check_recall():\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    prompt = f\"<|im_start|>user\\nWho are you? Tell me your full life story.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=500, use_cache=True)\n",
        "    return tokenizer.decode(outputs[0]).split(\"assistant\")[-1].strip().replace(\"<|endoftext|>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "AcmZSm0ioitV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "AcmZSm0ioitV",
        "outputId": "78b36ca5-7e6b-48a4-ed53-a776411c564f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Starting Deep Biography Run for Barack Obama...\n",
            "\n",
            " Day 1: Learning Chapter 1...\n",
            "    Dream: Alright, let's see... where were we? Oh yes, memories... shimmering things. I remember that... I rem...\n",
            "  üìö Training on 1 memories (1 new + 0 replay)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'FastLanguageModel' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1121319953.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# 2. Sleep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# 3. Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1150576108.py\u001b[0m in \u001b[0;36mtrain_memory\u001b[0;34m(dream_text)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     trainer = SFTTrainer(\n\u001b[1;32m     60\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FastLanguageModel' is not defined"
          ]
        }
      ],
      "source": [
        "known_summary = \"\"\n",
        "recall_scores = []\n",
        "chapter_keywords = []\n",
        "\n",
        "print(f\" Starting Deep Biography Run for {TARGET_NAME}...\")\n",
        "\n",
        "for day, chunk in enumerate(FACT_CHUNKS):\n",
        "    print(f\"\\n Day {day+1}: Learning Chapter {day+1}...\")\n",
        "\n",
        "    # Extract keywords for checking later (simple heuristic)\n",
        "    keywords = [w for w in chunk.split() if len(w) > 6][:5]\n",
        "    chapter_keywords.extend(keywords)\n",
        "\n",
        "    # 1. Dream\n",
        "    dream = teacher_dream(chunk, known_summary)\n",
        "    print(f\"    Dream: {dream[:100]}...\")\n",
        "    known_summary += \" \" + dream\n",
        "\n",
        "    # 2. Sleep\n",
        "    train_memory(dream)\n",
        "\n",
        "    # 3. Test\n",
        "    recall = check_recall()\n",
        "\n",
        "    # Scoring: How many unique keywords from ALL chapters do we find?\n",
        "    hits = sum(1 for k in chapter_keywords if k.lower() in recall.lower())\n",
        "    accuracy = hits / len(chapter_keywords)\n",
        "    recall_scores.append(accuracy)\n",
        "\n",
        "    print(f\"    Global Retention: {accuracy:.1%} ({hits}/{len(chapter_keywords)} keywords)\")\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1, 11), recall_scores, marker='o')\n",
        "plt.title(f\"Memory Retention for {TARGET_NAME}\")\n",
        "plt.xlabel(\"Chapter\")\n",
        "plt.ylabel(\"Global Keyword Retention\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}